{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 简答题",
   "id": "900a6e19bb0fa847"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. TensorFlow 是否可以简单替代 NumPy？两者之间的主要区别是什么？\n",
    "\n",
    "不能，numpy是立即执行，TensorFlow是通过图\n",
    "\n",
    "2. 使用 `tf.range(10)` 和 `tf.constant(np.arange(10))` 是否会得到相同的结果？\n",
    "\n",
    "会\n",
    "\n",
    "3. 可以通过编写函数或继承 `tf.keras.losses.Loss` 来定义自定义损失函数。两种方法分别应该在什么时候使用？\n",
    "\n",
    "函数适合不需要保存或配置参数的一次性使用的损失函数\n",
    "继承适合需要配置参数，要保存状态的可复用的复杂损失函数\n",
    "\n",
    "4. 可以直接在函数中定义自定义指标或采用 `tf.keras.metrics.Metric` 子类。两种方法分别应该在什么时候使用？\n",
    "\n",
    "函数适合简单一次性使用不累积的指标\n",
    "继承适合需要跨批次累积，需要重制状态的指标\n",
    "\n",
    "5. 什么时候应该自定义层而不是自定义模型？\n",
    "\n",
    "需要在不同模型中重复使用特定功能，需要自定义的前向传播和训练过程\n",
    "\n",
    "6. 有哪些示例需要编写自定义训练循环？\n",
    "\n",
    "多优化器训练，特殊的梯度操作\n",
    "\n",
    "7. 自定义 Keras 组件中可以包含任意 Python 代码，还是必须转换为 TF 函数？\n",
    "\n",
    "在图模式下，函数可以转换为 TF 函数，但必须使用 `@tf.function` 装饰器。\n",
    "\n",
    "8. 如果要将函数转换为 TF 函数，应避免哪些主要模式？\n",
    "\n",
    "避免使用python原生控制流，使用tensor类型\n",
    "\n",
    "9. 何时需要创建动态 Keras 模型？ 如何动态创建Keras模型？为什么不是所有模型都动态化？\n",
    "\n",
    "输入形状在运行时才能确定，大多数场景不需要\n"
   ],
   "id": "f0568883493d9ef7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 编程题",
   "id": "9a282b6d9adda052"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. 实现一个执行层归一化的自定义层：\n",
    "    - a. `build()` 方法应定义两个可训练的权重 α 和 β，它们的形状均为 `input_shape[-1:]`，数据类型为 `tf.float32`。α 应该用 1 初始化，而 β 必须用 0 初始化。\n",
    "    - b. `call()` 方法应计算每个实例特征的均值和标准差。为此，可以使用 `tf.nn.moments(inputs, axes=-1, keepdims=True)`，它返回同一实例的均值 μ 和方差 σ²（计算方差的平方根便可获得标准差）。然后，该函数应计算并返回\n",
    "      $$\n",
    "      \\alpha \\otimes \\frac{(X-\\mu)}{(\\sigma+\\epsilon)} + \\beta\n",
    "      $$\n",
    "      其中 ε 是表示项精度的一个常量（避免被零除的小常数，例如 0.001）,$\\otimes$表示逐个元素相乘\n",
    "    - c. 确保自定义层产生与tf.keras.layers.LayerNormalization层相同（或几乎相同）的输出。\n",
    "\n",
    "2. 使用自定义训练循环训练模型来处理Fashion MNIST数据集（13_神经网络介绍 里用的数据集）：\n",
    "\n",
    "    - a.显示每个轮次、迭代、平均训练损失和每个轮次的平均精度（在每次迭代中更新），以及每个轮次结束时的验证损失和精度。\n",
    "    - b.尝试对上面的层和下面的层使用具有不同学习率的不同优化器。"
   ],
   "id": "5cd3d7096f87afd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T12:30:59.684634Z",
     "start_time": "2025-09-14T12:30:59.677713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "class myLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.alpha = self.add_weight(name='alpha', shape=input_shape[-1:], dtype=tf.float32, initializer='ones', trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:], dtype=tf.float32, initializer='zeros', trainable=True)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, X):\n",
    "        mean, std2 = tf.nn.moments(X, axes=-1, keepdims=True)\n",
    "        std = tf.sqrt(std2)\n",
    "        n = (X - mean)/(std + 1e-3)\n",
    "        result = self.alpha * n + self.beta\n",
    "        return result\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units, \"activation\": tf.keras.activations.serialize(self.activation)}"
   ],
   "id": "4cb9da9f5e9ea249",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T12:35:22.980387Z",
     "start_time": "2025-09-14T12:35:22.550092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ],
   "id": "9a1d006c1f8b6be9",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T13:11:46.505430Z",
     "start_time": "2025-09-14T13:11:46.293728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    myLayer(100, activation='relu'),\n",
    "    myLayer(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ],
   "id": "1963286ca4affa33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\31752\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31752\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T13:39:51.633809Z",
     "start_time": "2025-09-14T13:39:51.629421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def random_batch(X,y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "\n",
    "def print_status_bar(step, total, loss, accuracy, metrics=None):\n",
    "    # 构建指标字符串，包含loss和accuracy\n",
    "    metrics_parts = [f\"{loss.name}: {loss.result():.4f}\", f\"accuracy: {accuracy.result():.4f}\"]\n",
    "    \n",
    "    # 添加其他指标（如果有的话）\n",
    "    if metrics:\n",
    "        for metric in metrics:\n",
    "            metrics_parts.append(f\"{metric.name}: {metric.result():.4f}\")\n",
    "    \n",
    "    metrics_str = \" - \".join(metrics_parts)\n",
    "    end = \"\" if step < total else \"\\n\"\n",
    "    print(f\"\\r{step}/{total} - \" + metrics_str, end=end)\n",
    "  "
   ],
   "id": "ce2271bdbfdec42c",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T13:39:52.021031Z",
     "start_time": "2025-09-14T13:39:52.010654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "upper_layer = model.get_layer(\"my_layer_1\")\n",
    "lower_layer = model.get_layer(\"my_layer\")\n",
    "upper_layer.trainable_variables"
   ],
   "id": "27ee3f2bd4e3302e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Variable path=sequential/my_layer_1/alpha, shape=(784,), dtype=float32, value=[1.0051452  1.0057523  1.0175804  1.0038154  0.99863946 0.9969828\n",
       "  0.99815476 1.009924   1.0194819  1.03052    1.0561574  1.0212556\n",
       "  1.0249522  1.0181849  1.0531468  1.0082369  1.0011609  1.0324738\n",
       "  1.019143   1.0699021  1.0170674  1.0216497  1.0047284  1.0124546\n",
       "  1.0036169  1.0046781  1.0156035  0.99404997 1.0086076  0.9904847\n",
       "  1.0026972  0.99379635 0.9973847  1.0161239  1.0321069  1.0009173\n",
       "  1.0055964  1.028414   1.0212828  1.0004395  1.0530629  0.99210227\n",
       "  0.9835926  0.98956543 1.0050054  1.0180683  1.0104201  1.0156282\n",
       "  1.0099325  1.0148365  1.0057255  1.0153649  1.0011528  0.98843366\n",
       "  0.9964974  0.9996796  1.0076636  0.98958397 1.0143368  1.002252\n",
       "  1.013004   0.9989664  1.0061308  1.010861   1.0266836  0.9979819\n",
       "  0.9908797  1.0209215  1.0133418  0.99241567 1.0173812  1.0192477\n",
       "  1.0225645  1.0057298  1.0118144  0.99173665 1.0172216  1.0162024\n",
       "  1.0199205  1.0239184  0.9935892  0.99184215 0.99162465 0.99957484\n",
       "  0.9929484  1.0118016  1.0052217  0.9974154  0.99998325 1.0128396\n",
       "  1.0004371  1.0083997  1.0304308  0.9906077  1.0017707  1.0055751\n",
       "  1.0010729  0.99778026 1.0081811  0.9892597  0.98424196 1.0090069\n",
       "  0.9959015  1.0140239  1.0030377  0.99118316 0.99695486 1.0015146\n",
       "  0.99884003 0.9959423  1.0027903  1.0149193  0.9925038  1.008112\n",
       "  0.99333376 0.9974564  1.0186373  0.98959804 1.0120976  0.99852794\n",
       "  0.9900634  1.0151364  1.0073143  0.99493355 1.017361   0.9916217\n",
       "  1.0033565  0.9889369  1.0184319  1.0013111  0.9967881  0.9901476\n",
       "  0.9966374  1.0033288  1.0036702  1.0299102  1.0096147  1.021989\n",
       "  1.0066066  0.99262345 0.9940123  0.99000615 1.0098691  0.9833336\n",
       "  1.0220312  0.99332273 0.9987505  1.007095   1.0077885  0.9977992\n",
       "  0.9950625  1.0102288  1.0134689  1.0092653  1.012938   1.000822\n",
       "  1.0140609  1.0046579  0.9973413  0.9959571  0.995002   0.99823076\n",
       "  1.0123798  1.0000001  1.0035034  1.0036502  1.0120815  1.0043867\n",
       "  1.0129375  0.9932379  1.013972   1.0039228  0.99760544 1.0054438\n",
       "  1.0163866  1.026667   1.0004851  1.0163351  1.01564    0.9988431\n",
       "  0.98476577 1.0085429  1.0359952  1.0220661  0.9928306  1.0001084\n",
       "  1.0086848  1.0224811  0.9880228  1.0033492  1.003988   1.0069827\n",
       "  1.0157509  1.0335411  0.9942275  1.0048559  0.9884712  0.9813528\n",
       "  1.0064465  1.010463   1.0056092  0.97893673 1.0091703  1.0020856\n",
       "  1.0196656  1.0008752  1.0040656  1.0243387  0.99497443 1.0095742\n",
       "  1.0057858  0.9989757  0.98919517 1.0010052  1.0266426  1.014923\n",
       "  0.99179494 0.98568994 0.99981964 0.9926595  0.9953259  1.0100032\n",
       "  1.0228008  1.0100278  0.9791903  0.99398875 1.0084987  1.014927\n",
       "  0.96726876 0.9823779  1.0126519  1.0229883  1.012632   1.012278\n",
       "  1.0367984  1.0088241  1.0004165  1.02755    1.0232491  1.0128646\n",
       "  0.9974564  1.0032455  0.9960296  0.9911514  1.0082715  0.9962891\n",
       "  0.99512887 1.0048497  1.0007699  1.0031338  1.0148036  0.97443014\n",
       "  1.0120095  0.97760594 0.9856532  1.0028405  1.0128837  1.0075369\n",
       "  1.0366349  1.0363353  0.98314786 1.0262827  1.0249135  1.0097766\n",
       "  1.012012   1.0061022  0.99230534 1.0014424  1.012764   1.0437206\n",
       "  1.0158225  0.9949402  0.98842204 0.99076104 0.9905941  0.9956558\n",
       "  1.0073506  1.0197582  0.99165374 1.0338097  1.0019628  1.0138022\n",
       "  0.9921275  1.0213464  1.0075794  1.0286202  1.0011346  1.0300511\n",
       "  1.0240376  1.0277867  0.9976831  0.99793893 0.990315   1.0030442\n",
       "  0.99785554 1.0139091  0.9905913  1.0121533  0.9905907  0.99926096\n",
       "  0.9926655  0.9993228  0.9898313  0.9957509  0.9905372  1.0179858\n",
       "  1.0300955  1.0331883  0.9929396  1.0234607  1.0100869  1.0037938\n",
       "  0.9979652  1.0426974  1.0243405  1.0269265  1.0362605  1.030513\n",
       "  1.000264   1.0293974  0.9945447  0.9983319  1.0056987  0.99573064\n",
       "  0.9925475  0.9917302  0.99576175 0.99357224 0.9921441  0.9888944\n",
       "  0.9940879  0.9846477  1.0043911  1.0018241  1.0050637  1.0121728\n",
       "  0.98968786 1.0179858  0.99868965 1.004535   0.9941775  0.98517114\n",
       "  1.0395694  1.0128345  1.012648   1.0034314  0.9901807  0.9860363\n",
       "  0.99319685 1.0020797  1.0030266  1.0085331  0.9953424  0.988131\n",
       "  1.0269969  0.99209523 1.00149    0.99290675 1.0268315  0.9723977\n",
       "  0.9922147  1.0090569  1.0002916  1.0226166  1.0218928  1.0166582\n",
       "  1.0209079  1.0088843  1.0020449  1.0294645  1.0133367  1.0275404\n",
       "  1.0450543  1.0078081  0.98781586 1.0174223  1.0078621  1.0008829\n",
       "  1.0158124  0.99823856 0.99173105 0.9884571  1.0121114  0.98958653\n",
       "  1.0339464  1.0138572  1.0046483  1.0023696  1.0071977  1.0174835\n",
       "  1.0165913  0.99660224 0.9934514  0.9839364  1.0047597  0.9968017\n",
       "  0.9983837  1.0414813  1.013471   1.03171    0.99279535 1.0086565\n",
       "  1.0006424  1.0140309  0.99560416 0.98366004 0.99158    1.0225946\n",
       "  1.0234997  0.9989737  0.98453444 0.98428154 0.9777569  1.0035821\n",
       "  0.99849343 0.9844269  1.0239066  1.0219195  1.0067562  0.99845874\n",
       "  1.0444044  1.0100877  1.0181788  1.0241199  0.99065536 1.0222062\n",
       "  0.99888384 1.016423   1.0027902  1.0084232  0.9931736  1.0021307\n",
       "  1.009308   0.9958513  1.010372   0.9985769  0.99453896 0.9883635\n",
       "  1.0125567  0.976781   1.0063422  0.9751854  0.99379086 0.990151\n",
       "  0.99251986 1.0346463  1.0239705  0.9983763  1.0033721  1.0209295\n",
       "  1.0126733  1.0478003  0.9904353  1.0054282  1.0072849  1.0027385\n",
       "  0.98644996 0.9988904  0.9920841  0.9944865  0.9876686  1.0070157\n",
       "  0.9833541  1.0142001  0.9871811  1.0031718  0.9707963  0.9964353\n",
       "  0.9886013  0.9899886  0.99218863 0.989786   1.0005486  0.9901661\n",
       "  0.99964935 1.001833   1.0210804  1.0109961  1.0423481  1.0273119\n",
       "  0.993737   1.0240451  0.98918056 0.99531376 0.9953821  0.98473215\n",
       "  1.0114019  1.0092859  0.9993512  1.0169451  0.99860513 0.9835235\n",
       "  0.9858712  1.0008761  0.98453647 0.9898702  1.0012126  0.989883\n",
       "  1.0066427  0.98768735 0.99228764 0.9926546  0.99954396 0.998285\n",
       "  1.0280502  0.99742    1.0263699  1.0110469  1.0119762  1.0116417\n",
       "  1.0181912  1.0120705  0.99919116 0.9924281  1.0007932  0.9931827\n",
       "  1.0010569  1.0133241  1.0096546  0.9779722  0.98081887 0.996394\n",
       "  0.9847027  0.9986669  1.0052698  1.015087   0.9873324  0.99474806\n",
       "  0.9945366  0.98947763 0.9926565  1.0005504  1.0076933  0.9901892\n",
       "  1.0196635  1.0194438  0.98967993 1.011271   0.9987186  0.9901087\n",
       "  1.0069348  0.99084544 1.0092481  0.99371    1.004479   1.0063453\n",
       "  1.0283155  1.0010488  1.0001816  0.99259335 0.99381894 0.9889164\n",
       "  0.9888782  0.9822496  0.991312   1.0038141  0.9930293  1.013019\n",
       "  1.0152961  1.0279738  1.001905   0.9995438  1.0020925  1.0009636\n",
       "  0.99642783 1.0644119  1.0046495  1.011986   0.9857697  0.99904674\n",
       "  1.0020063  1.0084367  0.99971855 0.99692667 1.0339586  0.9815831\n",
       "  1.0161524  1.0105693  1.0055352  0.99576396 0.998231   0.996914\n",
       "  0.9898929  1.0000424  1.010684   1.0304502  1.0021523  1.0118005\n",
       "  0.9829369  1.004517   0.9837857  0.9979586  0.98491085 0.99961096\n",
       "  0.9825498  1.002291   0.9902506  0.99259496 1.0040756  0.990905\n",
       "  1.0005927  0.9963464  1.0200461  1.0105352  1.0020767  1.0029005\n",
       "  1.0374354  1.0039955  0.98724836 0.99605066 1.00706    1.0018107\n",
       "  0.9941395  1.032955   0.99010295 0.9953602  1.0057257  0.9980565\n",
       "  1.0129949  1.000236   0.9872913  0.99894625 0.9892945  0.98992956\n",
       "  0.99932885 0.9806298  0.990409   1.0060761  1.0055468  0.99773353\n",
       "  1.0162383  0.9898302  0.9936122  1.0154217  1.0722837  0.97921526\n",
       "  0.99707204 0.9878598  0.9925705  1.0126297  0.9982552  0.9976106\n",
       "  0.9969896  0.99369323 1.0022137  0.99753344 0.99521625 0.99659675\n",
       "  0.9991756  0.9990293  0.98334473 0.9948772  1.0255588  0.9979687\n",
       "  0.9883507  1.0027425  1.0077455  1.0038315  1.004975   0.99857354\n",
       "  0.989207   1.0051246  1.0258595  1.0025309  0.9940742  0.9880211\n",
       "  0.99273556 0.9994066  1.0069704  1.0067065  1.0043119  0.99727625\n",
       "  0.9989254  0.9935518  0.9992027  1.0012082  1.0110582  1.0014278\n",
       "  1.0013968  0.99124205 1.0133158  0.99993235 0.99702483 0.9891144\n",
       "  1.0065104  0.9937088  1.0095863  0.98892385 0.99969923 1.0059247\n",
       "  1.0145377  0.9940559  0.99935263 0.9987755  0.9787797  1.0141506\n",
       "  1.00652    0.98953736 0.99692965 0.99309874 0.97604114 1.0081279\n",
       "  1.0039579  0.995805   1.000602   0.9970112  0.9930259  0.9948951\n",
       "  0.9903726  1.0127743  0.9962802  0.98607236 0.99588734 1.0199643\n",
       "  1.0102856  0.99629366 0.9909709  1.0039688  1.0058137  1.0183451\n",
       "  0.9981017  0.9937886  0.97763735 0.99717224 0.99440616 1.0010567\n",
       "  0.9883895  1.0003979  1.0027246  1.011244   1.0163274  1.0056356\n",
       "  1.0055808  1.0064492  0.9921245  1.0043802  1.0347997  0.99515736\n",
       "  1.005154   0.9898833  1.0111835  0.99631417 1.0308175  0.9944135\n",
       "  1.0086551  1.0144742  0.99659    1.0083792  1.0145174  1.0044354\n",
       "  1.0120361  1.0050297  1.0047133  1.0024654  0.99252754 0.9836775\n",
       "  0.9878905  1.0157918  0.9841511  0.9849544  1.0071718  1.0000625\n",
       "  1.000862   1.0134888  1.0369731  1.0392953  1.0458274  1.0054715\n",
       "  1.0098021  1.0654455  1.0893618  1.0097299  0.99717224 1.0083342\n",
       "  1.0075694  0.9950508  1.0450858  1.0213941  1.0049707  1.0106896\n",
       "  1.0078756  1.0133988  1.0088028  0.99282974]>,\n",
       " <Variable path=sequential/my_layer_1/beta, shape=(784,), dtype=float32, value=[ 8.38095788e-03  1.90625153e-02  2.36612502e-02 -3.42552108e-03\n",
       "   2.47388594e-02  3.49271344e-03  1.10616190e-02  1.34281861e-02\n",
       "  -1.05083296e-02  6.52120728e-03  7.74069736e-03  1.51426727e-02\n",
       "  -1.83621757e-02 -4.22090106e-02 -7.46491849e-02 -2.74958042e-03\n",
       "  -1.29977316e-02 -1.84017029e-02  2.14210022e-02  4.06479053e-02\n",
       "  -1.70860160e-02 -8.32927506e-03  5.12473658e-03  2.07859632e-02\n",
       "   3.85040417e-03  1.79008152e-02  1.90202463e-02 -3.97383124e-02\n",
       "   1.33925620e-02 -2.06657276e-02  1.25698736e-02  1.05678216e-02\n",
       "  -9.99029726e-03  2.47247610e-02  2.09677480e-02  2.88017262e-02\n",
       "  -1.82175785e-02  1.48967663e-02  5.56777418e-03 -1.83254145e-02\n",
       "   4.71191248e-03 -2.95187104e-02 -2.62414273e-02 -2.83317827e-03\n",
       "  -4.78254594e-02 -4.91067283e-02  3.55342776e-02  1.60540827e-02\n",
       "   1.68443397e-02  8.04939773e-03  9.51430702e-04  2.17972789e-03\n",
       "  -7.78209869e-05 -2.01504678e-02 -1.10054640e-02 -3.49609293e-02\n",
       "   2.24377476e-02 -9.02601890e-03  3.50336321e-02  2.27596257e-02\n",
       "   4.08906266e-02  2.33049095e-02  1.21999253e-02  2.66847294e-02\n",
       "   3.05583421e-02  3.06739770e-02  2.11740155e-02 -6.24729041e-03\n",
       "   1.75000913e-02 -3.77166793e-02 -5.44575825e-02  3.09561193e-02\n",
       "  -2.83942912e-02  6.05669059e-03 -2.58271843e-02 -1.58245284e-02\n",
       "   3.20290476e-02  6.55970499e-02  4.45675999e-02  2.49215905e-02\n",
       "  -1.94970295e-02 -1.75627600e-02  4.04908927e-03 -2.49093901e-02\n",
       "  -2.98566651e-02  2.88112238e-02  2.15133280e-02 -6.49680197e-03\n",
       "   1.29692191e-02  5.20587489e-02  2.55871359e-02  1.51845533e-02\n",
       "   5.56586459e-02  6.67486107e-03 -5.24452850e-02  5.89735247e-03\n",
       "  -2.32018325e-02 -3.43011022e-02 -4.28568795e-02 -9.43785720e-03\n",
       "  -4.24059434e-03  1.19551923e-02 -2.04820372e-02  1.29039474e-02\n",
       "   2.91265324e-02  2.71494407e-02  7.49220932e-03  3.57800280e-04\n",
       "  -3.91876623e-02 -1.42786829e-02  1.90746970e-02  3.27980556e-02\n",
       "   2.45190575e-03  2.18514726e-02 -2.26695165e-02 -1.68435033e-02\n",
       "  -2.22560279e-02  2.63547283e-02 -6.68011652e-03 -2.21739244e-02\n",
       "  -4.42995839e-02  1.00918757e-02 -2.73901336e-02 -4.47378829e-02\n",
       "   2.73398031e-02 -3.70719507e-02 -4.61798459e-02 -1.17132831e-02\n",
       "  -2.79378314e-02  1.45516293e-02  4.88338480e-03  2.44138595e-02\n",
       "   2.71092751e-04  8.39352235e-02  4.63623255e-02  2.42846133e-03\n",
       "   2.42767651e-02 -7.58936629e-04  9.29673668e-04 -3.17001007e-02\n",
       "   3.48850549e-03 -1.71757676e-02  2.40094792e-02 -1.62888039e-02\n",
       "   4.30380702e-02 -2.06407160e-02 -1.38602611e-02 -1.34419594e-02\n",
       "   1.20969005e-02 -3.41600813e-02  4.76644933e-03 -3.71499807e-02\n",
       "   7.64119904e-03 -3.27067040e-02  1.12065615e-03 -7.07493797e-02\n",
       "  -6.60452917e-02 -3.00806575e-03 -9.53331310e-03  1.48290042e-02\n",
       "  -2.83031561e-03  3.40591883e-03  4.86731306e-02  2.39823014e-02\n",
       "   4.18774672e-02 -7.93822855e-03 -8.36721715e-03  1.80532802e-02\n",
       "   3.09084225e-02  4.55178693e-03  1.39095848e-02  1.52703468e-02\n",
       "   3.37907784e-02 -2.10643038e-02  3.25535163e-02 -2.57690549e-02\n",
       "  -3.67308781e-02 -4.04781736e-02  1.74942221e-02  1.34933665e-02\n",
       "  -9.15592723e-03  2.12877207e-02 -1.28025273e-02 -2.56070755e-02\n",
       "  -2.77802930e-03  1.24491879e-03  1.38726197e-02 -3.59674133e-02\n",
       "  -1.90338995e-02 -1.23778423e-02  3.53347845e-02  3.61359231e-02\n",
       "   4.10752259e-02  3.32031175e-02 -3.07334904e-02  1.57549344e-02\n",
       "  -3.09102666e-02 -5.14029227e-02  4.46456410e-02 -2.00798530e-02\n",
       "  -2.24391092e-03 -4.87998407e-03 -1.00124422e-02 -1.13923363e-02\n",
       "  -3.35033573e-02 -5.08774482e-02 -3.08327936e-02 -5.23862801e-03\n",
       "   6.26153871e-03 -8.96839425e-03  3.72990919e-03 -9.21937579e-04\n",
       "  -3.44654312e-04 -2.29544453e-02 -4.04077992e-02 -3.66607197e-02\n",
       "   3.41008417e-02  6.44011050e-03  2.04835031e-02 -7.50536611e-03\n",
       "   1.37730897e-03 -8.56280141e-03  2.13773455e-02  8.11621174e-03\n",
       "  -3.90175916e-02 -1.25678058e-03  1.80934612e-02  2.31863242e-02\n",
       "  -8.41186941e-03 -9.26431641e-03 -2.41859742e-02  5.29086357e-03\n",
       "  -8.09249189e-03 -2.39846320e-03  7.72250444e-03 -6.17194586e-02\n",
       "  -1.42487781e-02  3.47494185e-02  7.04085920e-04  2.47255364e-03\n",
       "  -2.32473519e-02 -3.63850743e-02 -3.56335156e-02 -6.88611716e-03\n",
       "   5.39516658e-03  2.49878578e-02 -1.07624987e-02  1.34078478e-02\n",
       "   9.40355007e-03 -3.01497541e-02  1.08494312e-02 -4.53425646e-02\n",
       "   4.20662947e-02 -2.63110064e-02 -3.19361053e-02  3.35080624e-02\n",
       "  -1.85900349e-02 -1.75368339e-02 -1.06166564e-02 -6.47181422e-02\n",
       "   1.81137975e-02 -2.11079568e-02  1.78088043e-02 -5.61192399e-03\n",
       "   4.37087333e-03  7.90313073e-03  3.53494398e-02 -1.37299001e-02\n",
       "   2.37573311e-03 -1.96883027e-02 -2.69759241e-02  2.14246083e-02\n",
       "  -9.63861065e-04 -1.64876804e-02  4.29127924e-02  9.80175217e-04\n",
       "   8.50840937e-03  2.71454249e-02  1.34946061e-02  5.01960889e-02\n",
       "   3.33963484e-02  2.03402340e-02  7.64417136e-03 -6.80152606e-03\n",
       "  -3.67102884e-02 -1.98763013e-02 -4.59384732e-02  1.99337713e-02\n",
       "  -2.33480446e-02 -3.96921794e-04  7.21534109e-03 -8.45967699e-03\n",
       "   2.68684775e-02 -1.44264428e-02 -1.21319601e-02 -3.41197811e-02\n",
       "   4.26364643e-03 -8.22870154e-03  1.07717756e-02 -3.06703746e-02\n",
       "   2.30072495e-02 -1.96670983e-02 -3.13284509e-02  1.08226305e-02\n",
       "   2.12872252e-02 -5.19398190e-02  4.83853519e-02  2.36762390e-02\n",
       "  -2.81629208e-02  1.23005537e-02  4.37074229e-02  3.14568169e-03\n",
       "   1.44771289e-03 -2.37676334e-02 -1.06644342e-02  1.33632366e-02\n",
       "  -2.41095107e-02 -7.49685860e-04  1.51963029e-02  9.31009836e-03\n",
       "   2.06702109e-03 -1.57778487e-02 -3.94376144e-02 -3.20284441e-03\n",
       "  -1.11040333e-02 -3.30379270e-02  8.33800994e-03 -6.74399547e-03\n",
       "   1.44883664e-03 -4.82252100e-03 -1.79004960e-03 -2.58873915e-03\n",
       "   9.55487043e-03  3.96474265e-03  2.08635163e-03  3.50878909e-02\n",
       "  -1.39484257e-02  3.42740938e-02  1.54105080e-02 -6.72448892e-03\n",
       "  -2.10570488e-02 -9.12686437e-03  3.29384394e-02  3.68006118e-02\n",
       "   2.17262190e-02  4.69201524e-03  2.86815339e-03 -6.89137541e-03\n",
       "   1.61736086e-02 -2.44648363e-02 -4.55813333e-02 -1.98040958e-02\n",
       "   9.64718685e-03 -7.48320855e-03 -3.96109447e-02  2.28775218e-02\n",
       "  -1.65747274e-02 -3.78427305e-03 -2.96018254e-02  1.79925449e-02\n",
       "  -5.22219576e-03 -8.72095395e-03  1.22794257e-02  4.70545255e-02\n",
       "   5.91915846e-02  3.67028601e-02  5.51644824e-02 -2.98153833e-02\n",
       "  -2.68220063e-02  1.69699453e-02  3.00665442e-02  5.32985255e-02\n",
       "  -3.20878229e-04  5.31671420e-02  1.10614439e-02 -4.91705015e-02\n",
       "  -2.73634195e-02  2.16292497e-03 -4.67152819e-02  1.64852273e-02\n",
       "   5.51788043e-03  5.66286850e-04 -3.26442458e-02  9.10366233e-03\n",
       "  -5.24201654e-02 -1.42673030e-02 -2.37689950e-02 -1.65056661e-02\n",
       "  -8.26399121e-03 -2.71892902e-02 -1.40296733e-02  1.84190972e-03\n",
       "  -5.07280510e-03 -2.29815226e-02 -4.41793166e-02 -4.52090167e-02\n",
       "   3.11948080e-02  7.50912875e-02  9.50376270e-04  1.66976973e-02\n",
       "   6.28429698e-03  6.91690156e-03  1.77811719e-02 -3.80185135e-02\n",
       "   2.58824293e-04 -9.88832489e-03  1.85579329e-03 -5.28658219e-02\n",
       "  -2.74922308e-02 -1.94006115e-02 -1.29987311e-04  7.83115905e-03\n",
       "   3.02015804e-02 -6.48700725e-03 -2.11874619e-02  2.11008601e-02\n",
       "  -1.31119583e-02 -1.11487415e-02 -9.38152603e-04  1.12609752e-02\n",
       "   7.57514164e-02 -1.24754486e-04 -2.45937593e-02 -8.17305595e-02\n",
       "   2.50745751e-02  5.93609326e-02  3.99909914e-02  2.63946280e-02\n",
       "   4.66398969e-02  2.19171718e-02 -1.14608854e-02 -1.00314561e-02\n",
       "  -3.01204082e-02 -1.25001771e-02  1.94437383e-03 -7.97468703e-03\n",
       "   9.46137588e-03  1.48159731e-02 -2.13664435e-02  4.24474105e-03\n",
       "  -3.49140191e-03  6.05689478e-04  5.28994342e-03 -9.37200058e-03\n",
       "  -1.29950838e-02  7.09331222e-03 -6.62425533e-03 -8.91034491e-03\n",
       "  -7.33886007e-03  1.94540489e-02 -2.39088684e-02 -2.25082263e-02\n",
       "  -4.39435840e-02  2.73961276e-02  4.51930352e-02  7.52843097e-02\n",
       "   2.64149550e-02  1.21049592e-02  1.88570693e-02 -9.75674670e-03\n",
       "  -1.34755559e-02 -2.55334619e-02  9.82022099e-03  1.10344577e-03\n",
       "   1.78370066e-02 -2.59453431e-03  2.15294082e-02 -6.51222141e-03\n",
       "   1.32007971e-02 -2.40251725e-03 -4.93225595e-03  2.29267008e-03\n",
       "  -8.24780576e-03  4.94998768e-02  1.28615806e-02 -3.93161029e-02\n",
       "   3.64248380e-02  7.58377928e-03 -3.36159281e-02 -4.47832122e-02\n",
       "   5.06937737e-03  6.20151572e-02  4.95012105e-02  1.94756277e-02\n",
       "   1.22886449e-02  4.03950401e-02 -5.11574782e-02 -1.98886544e-02\n",
       "  -1.05601577e-02 -2.85549611e-02  7.63132935e-03  2.17306186e-02\n",
       "  -2.96713673e-02 -1.16887810e-02  3.49190980e-02  5.90209058e-03\n",
       "   1.36861401e-02  2.21743677e-02 -2.69530378e-02  1.37354070e-02\n",
       "   2.46433578e-02  3.36602218e-02  4.15348671e-02  2.31474862e-02\n",
       "   4.12795022e-02 -6.29771035e-03 -7.02920705e-02 -4.13564481e-02\n",
       "   2.07398105e-02  3.42980176e-02  4.40870747e-02  1.45977894e-02\n",
       "   6.44260040e-03  1.92610528e-02 -1.46158533e-02 -4.27149143e-03\n",
       "  -3.24342325e-02 -2.10187584e-02  2.07605045e-02 -1.84732699e-03\n",
       "   1.77001785e-02 -1.61116179e-02  1.12301353e-02 -2.43301187e-02\n",
       "  -2.08516791e-02 -3.54730710e-02  1.66203361e-02  1.87387820e-02\n",
       "   4.78027165e-02  2.87706740e-02  1.00044440e-02  5.73883159e-03\n",
       "   2.04451997e-02  8.07574857e-03 -7.04950560e-03 -6.41887337e-02\n",
       "  -2.56566070e-02  4.25997078e-02  3.64977606e-02  3.39857042e-02\n",
       "   2.08875760e-02  3.58273345e-03 -2.26297379e-02 -2.16193274e-02\n",
       "  -2.96758376e-02 -3.14963385e-02 -3.10346093e-02 -2.86528585e-03\n",
       "  -2.19703857e-02  5.64694544e-03  1.78282931e-02 -1.67215299e-02\n",
       "   7.06391921e-03  7.98589550e-03 -3.31509374e-02 -1.37376962e-02\n",
       "   3.66758443e-02  4.88403440e-02 -2.65231682e-03 -2.19524489e-03\n",
       "   1.99649371e-02  6.64069178e-03  3.61593347e-03 -1.70780830e-02\n",
       "  -2.57828329e-02  7.61084957e-03  9.35436040e-03  1.94336884e-02\n",
       "  -1.44679258e-02  1.99130867e-02 -3.30427065e-02 -8.33791401e-03\n",
       "   1.20677485e-03  1.65819954e-02 -2.49380060e-02  1.27845388e-02\n",
       "  -3.62453498e-02 -8.87245871e-03 -2.35221609e-02 -1.84274409e-02\n",
       "  -4.56154570e-02  1.68781076e-02 -3.28973727e-03 -6.27512531e-03\n",
       "   2.19841823e-02  5.50160222e-02 -1.66229496e-03  1.87473036e-02\n",
       "  -4.35583517e-02 -8.98859743e-03 -1.86489690e-02 -3.40832770e-02\n",
       "  -1.33393530e-03  5.45451359e-04 -4.60266043e-03  3.96905653e-02\n",
       "  -3.16053920e-04 -1.95266418e-02  2.22227117e-03  1.44934980e-02\n",
       "  -1.09136738e-02 -1.01066036e-02 -1.93415973e-02  3.68656334e-03\n",
       "   2.76060533e-02 -1.00127165e-03 -3.50667015e-02 -4.29298729e-02\n",
       "  -3.50947194e-02 -2.46217661e-02 -9.75112431e-03 -2.28125998e-03\n",
       "  -2.82202326e-02  1.57845225e-02  8.80226400e-03  1.54526848e-02\n",
       "   3.21304053e-02 -1.98740279e-03 -1.11598102e-02 -2.60551809e-03\n",
       "   7.99027737e-03  1.48486467e-02  2.52441112e-02  2.22111419e-02\n",
       "   2.71761004e-04 -7.65816914e-03  6.60891738e-03  1.65727474e-02\n",
       "  -4.16237302e-03 -4.94618027e-04  2.81212572e-02 -3.92920300e-02\n",
       "  -1.43627316e-04 -1.48386089e-02 -1.86197553e-02 -1.51098100e-02\n",
       "  -1.00686345e-02 -4.23003919e-03  2.09857021e-02 -4.17881366e-03\n",
       "  -2.89256894e-03  3.30386162e-02  1.51989963e-02  1.37358364e-02\n",
       "   5.17465873e-03 -1.68830380e-02  3.37065868e-02 -2.21655015e-02\n",
       "  -1.30356904e-02  1.28800021e-02 -1.97423473e-02  4.02455702e-02\n",
       "   2.94804461e-02  2.90603423e-03  2.37222407e-02  1.76773220e-02\n",
       "  -3.83533025e-03 -1.58004332e-02  1.28448233e-02  2.63514761e-02\n",
       "  -2.65532546e-02  2.51247361e-03 -1.30960187e-02  6.82922732e-03\n",
       "  -5.70350327e-04 -2.75331661e-02 -9.40841716e-03 -2.14941315e-02\n",
       "  -6.97281503e-04  2.96203792e-02  1.46719059e-02  4.22520423e-03\n",
       "  -1.75781769e-03  1.19292792e-02  1.56886857e-02 -9.69455345e-04\n",
       "  -3.32999825e-02  3.56389619e-02  4.06901278e-02 -9.92986746e-03\n",
       "   3.67893204e-02  4.34484407e-02  1.77168138e-02  1.57561363e-03\n",
       "   2.80651706e-03 -3.30154262e-02  1.75359026e-02  2.09388584e-02\n",
       "  -1.55006433e-02 -2.01892592e-02  7.68699218e-03  1.02465888e-02\n",
       "  -3.34692672e-02 -4.26246263e-02  1.22995712e-02 -3.41417082e-02\n",
       "   7.66053749e-03 -2.62540597e-02  7.47263664e-03 -3.35620306e-02\n",
       "  -4.42109630e-02  4.19197567e-02 -1.29436404e-02 -1.47550320e-02\n",
       "   1.48399512e-03  2.13709399e-02  1.20182950e-02 -1.16149662e-02\n",
       "   8.64354987e-03  6.85461983e-02 -1.95582788e-02 -2.44941823e-02\n",
       "   1.66626927e-02  9.56592802e-03  1.00104781e-02 -3.51922885e-02\n",
       "  -3.89157911e-03 -5.16347662e-02 -1.68626700e-02  3.08864210e-02\n",
       "   3.02426214e-03 -1.03468588e-02  5.12505099e-02 -4.51044478e-02\n",
       "  -2.06822604e-02  1.31293274e-02 -2.68069971e-02  1.71481539e-02\n",
       "   2.28443332e-02  3.37832756e-02  2.93378308e-02  3.54550011e-03\n",
       "   1.21790804e-02  1.63116623e-02  4.15417552e-03 -2.62628100e-03\n",
       "   4.73154411e-02  8.31204187e-03  2.72188671e-02  4.75086272e-03\n",
       "   3.35085235e-04 -1.12680439e-02  1.73413649e-03  1.57822240e-02\n",
       "  -9.53169074e-03  1.24504361e-02  1.31767960e-02  3.13561037e-02\n",
       "  -4.61122952e-03  1.16076190e-02 -3.81432623e-02  5.47830481e-04\n",
       "  -4.26522398e-04  4.19452675e-02 -1.56874359e-02 -8.28509161e-04\n",
       "  -1.34623749e-02  4.78860140e-02 -6.13641292e-02 -3.49318720e-02\n",
       "  -1.38876878e-03 -1.77121535e-02 -2.49079918e-03  2.56800465e-02\n",
       "   2.91985292e-02  2.71380413e-02  5.23658991e-02 -2.97569763e-02\n",
       "   2.06961669e-02 -3.57967778e-03 -1.88103747e-02 -1.04034543e-02\n",
       "   1.47219573e-04  1.92211121e-02  1.90608166e-02 -3.02043594e-02\n",
       "   3.57420295e-02 -4.64200880e-03  2.31001340e-02  1.44563417e-03\n",
       "   1.23828938e-02  3.14445458e-02  2.81566996e-02 -4.32447121e-02]>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T13:41:33.458014Z",
     "start_time": "2025-09-14T13:39:52.951510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "\n",
    "optimizer1 = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "optimizer2 = tf.keras.optimizers.SGD(learning_rate=0.05)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "train_loss = tf.keras.metrics.Mean(name=\"mean_loss\")\n",
    "val_loss = tf.keras.metrics.Mean(name=\"val_loss\")\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"val_accuracy\")\n",
    "metrics = [tf.keras.metrics.SparseCategoricalCrossentropy()]\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{n_epochs}\")\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train, y_train)\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            y_pred = model(X_batch, training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))  # \n",
    "            loss = tf.add_n([main_loss] + model.losses)  \n",
    "        upper_layer = model.get_layer(\"my_layer_1\")\n",
    "        lower_layer = model.get_layer(\"my_layer\")\n",
    "        gradients1 = tape.gradient(loss, upper_layer.trainable_variables)\n",
    "        gradients2 = tape.gradient(loss, lower_layer.trainable_variables)\n",
    "        del tape\n",
    "        \n",
    "        optimizer1.apply_gradients(zip(gradients1, upper_layer.trainable_variables))\n",
    "        optimizer2.apply_gradients(zip(gradients2, lower_layer.trainable_variables))\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "\n",
    "        train_loss(loss)  \n",
    "        train_accuracy(y_batch, y_pred)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step, n_steps, train_loss, train_accuracy, metrics)\n",
    "    \n",
    "    val_loss(model(X_val, training=False))\n",
    "    val_accuracy(y_val, model(X_val, training=False))\n",
    "    print(f\"Validation: loss: {val_loss.result():.4f}, accuracy: {val_accuracy.result():.4f}\")\n",
    "    for metric in [train_loss] + metrics:\n",
    "        metric.reset_state()  "
   ],
   "id": "671d1b54a2c6fd49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 - mean_loss: 0.3948 - accuracy: 0.8639 - sparse_categorical_crossentropy: 0.3948\n",
      "Validation: loss: 0.1000, accuracy: 0.8480\n",
      "Epoch 2/5\n",
      "1500/1500 - mean_loss: 0.3997 - accuracy: 0.8633 - sparse_categorical_crossentropy: 0.3997\n",
      "Validation: loss: 0.1000, accuracy: 0.8481\n",
      "Epoch 3/5\n",
      "1500/1500 - mean_loss: 0.3927 - accuracy: 0.8640 - sparse_categorical_crossentropy: 0.3927\n",
      "Validation: loss: 0.1000, accuracy: 0.8483\n",
      "Epoch 4/5\n",
      "1500/1500 - mean_loss: 0.3943 - accuracy: 0.8639 - sparse_categorical_crossentropy: 0.3943\n",
      "Validation: loss: 0.1000, accuracy: 0.8483\n",
      "Epoch 5/5\n",
      "1500/1500 - mean_loss: 0.3919 - accuracy: 0.8637 - sparse_categorical_crossentropy: 0.3919\n",
      "Validation: loss: 0.1000, accuracy: 0.8485\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\n",
   "id": "5bd1601b79706b98"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

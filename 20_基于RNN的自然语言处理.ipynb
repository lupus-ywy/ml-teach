{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 使用字符RNN生成莎士比亚文本",
   "id": "b6905ec0806fdce8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "在2015年的一篇著名博客文章 \"https://karpathy.github.io/2015/05/21/rnn-effectiveness/\" 中，Andrej Karpathy展示了如何训练循环神经网络来预测句子中的下一个字符。这个char-RNN可以用来生成小说文本，每次一个字符。",
   "id": "bdf100365ed041e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-14T00:34:08.916987Z",
     "start_time": "2025-10-14T00:34:02.365364Z"
    }
   },
   "source": [
    "# tf.keras.utils.get_file()函数下载莎士比亚的所有作品\n",
    "import tensorflow as tf\n",
    "\n",
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T00:34:37.642735Z",
     "start_time": "2025-10-14T00:34:37.638439Z"
    }
   },
   "cell_type": "code",
   "source": "print(shakespeare_text[:80])",
   "id": "5a0427af95a99073",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T00:34:38.858017Z",
     "start_time": "2025-10-14T00:34:38.242110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用tf.keras.layers.TextVectorization层对此文本进行编码。我们将设置split=\"character\"，基于字符进行编码，而不是默认的基于单词进行编码，并使用standardize=\"lower\"将文本转换为小写（这将简化任务）：\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\", standardize=\"lower\")\n",
    "\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "\n",
    "\n",
    "encoded = text_vec_layer([shakespeare_text])[0]\n",
    "encoded\n"
   ],
   "id": "a051c0bf83bb06c5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([21,  7, 10, ..., 22, 28, 12], dtype=int64)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T00:34:39.558563Z",
     "start_time": "2025-10-14T00:34:39.552201Z"
    }
   },
   "cell_type": "code",
   "source": "text_vec_layer.get_vocabulary()",
   "id": "fc62a9636f6ee7f3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " ' ',\n",
       " 'e',\n",
       " 't',\n",
       " 'o',\n",
       " 'a',\n",
       " 'i',\n",
       " 'h',\n",
       " 's',\n",
       " 'r',\n",
       " 'n',\n",
       " '\\n',\n",
       " 'l',\n",
       " 'd',\n",
       " 'u',\n",
       " 'm',\n",
       " 'y',\n",
       " 'w',\n",
       " ',',\n",
       " 'c',\n",
       " 'f',\n",
       " 'g',\n",
       " 'b',\n",
       " 'p',\n",
       " ':',\n",
       " 'k',\n",
       " 'v',\n",
       " '.',\n",
       " \"'\",\n",
       " ';',\n",
       " '?',\n",
       " '!',\n",
       " '-',\n",
       " 'j',\n",
       " 'q',\n",
       " 'x',\n",
       " 'z',\n",
       " '3',\n",
       " '&',\n",
       " '$']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T00:34:40.161950Z",
     "start_time": "2025-10-14T00:34:40.155804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 每个字符现在都映射到一个整数，从2开始。TextVectorization层将值0保留为填充词元(token)，并将值1保留为未知字符。\n",
    "# 现在不需要这两个词元，因此从字符ID数中减去2，并计算不同字符的总数和总字符数：\n",
    "encoded -= 2\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
    "dataset_size = len(encoded)\n",
    "\n",
    "print(n_tokens)"
   ],
   "id": "8e7d446d727dc90e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "将这个非常长的序列转换成一个窗口数据集，然后使用它来训练一个序列到序列的循环神经网络。目标序列将类似于输入序列，但是会向“未来”移动一个时间步。例如，数据集中的样本可能是一个由表示文本“to be or not to b”（不包括最后一个“e”）的字符ID组成的序列，相应的目标序列是一个由表示文本“o be or not to be”（包括最后一个“e”，但不包括开头的“t”）的字符ID组成的序列",
   "id": "6f9c13d7635eb999"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T00:34:41.488712Z",
     "start_time": "2025-10-14T00:34:41.483888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 将字符ID序列转换成输入/目标窗口对的数据集\n",
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ],
   "id": "70ed9e8982001bf0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 它将序列作为输入（即编码文本），并创建一个包含所需长度的所有窗口的数据集。\n",
    "- 它将长度增加1，因为我们需要将下一个字符放在目标序列中。\n",
    "- 然后，它对窗口进行乱序处理（可选），将它们分批，将它们拆分为输入/输出对，并激活预取功能。\n",
    "\n",
    "长度为11的窗口和批量大小3。每个窗口的起始索引都显示在旁边：\n",
    "\n",
    "![准备乱序窗口的数据集](./images/RNN/p7.png)"
   ],
   "id": "205152dc0a2c913e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T00:34:43.418817Z",
     "start_time": "2025-10-14T00:34:43.324915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 大约90%的文本进行训练，5%用于验证，5%用于测试\n",
    "length = 100  # length决定循环神经网络能学习的最长模式\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True, seed=42)\n",
    "valid_set = to_dataset(encoded[1_000_000: 1_060_000], length=length)\n",
    "test_set = to_dataset(encoded[1_060_000:], length=length)"
   ],
   "id": "414ec5b323fe4c3",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T00:34:44.008430Z",
     "start_time": "2025-10-14T00:34:43.896037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for data in valid_set.take(1):\n",
    "    print(data)"
   ],
   "id": "2198babd695e6b80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 100), dtype=int64, numpy=\n",
      "array([[ 5,  7,  0, ...,  6,  1,  0],\n",
      "       [ 7,  0,  8, ...,  1,  0, 18],\n",
      "       [ 0,  8,  1, ...,  0, 18,  6],\n",
      "       ...,\n",
      "       [ 4,  2,  0, ...,  3, 26, 10],\n",
      "       [ 2,  0,  7, ..., 26, 10, 10],\n",
      "       [ 0,  7,  6, ..., 10, 10,  2]], dtype=int64)>, <tf.Tensor: shape=(32, 100), dtype=int64, numpy=\n",
      "array([[ 7,  0,  8, ...,  1,  0, 18],\n",
      "       [ 0,  8,  1, ...,  0, 18,  6],\n",
      "       [ 8,  1,  4, ..., 18,  6,  3],\n",
      "       ...,\n",
      "       [ 2,  0,  7, ..., 26, 10, 10],\n",
      "       [ 0,  7,  6, ..., 10, 10,  2],\n",
      "       [ 7,  6,  1, ..., 10,  2,  8]], dtype=int64)>)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 构建和训练char-RNN模型",
   "id": "ce01571b7082bc92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 使用Embedding层作为第一层，以编码字符ID。Embedding层的输入维数是不同字符ID的数量，输出维数是可以调整的超参数——现在将其设置为16。Embedding层的输入将是形状为［批量大小，窗口长度］的二维张量，Embedding层的输出将是形状为［批量大小，窗口长度，嵌入大小］的三维张量。\n",
    "- 为输出层使用Dense层：它必须具有39个单元(n_tokens)，因为文本中有39个不同的字符，希望在每个时间步输出每个可能字符的概率。每个时间步39个输出概率的总和应该为1，因此对Dense层的输出应用softmax激活函数。\n",
    "- 最后，使用\"sparse_categorical_crossentropy\"损失和Nadam优化器编译此模型，并使用ModelCheckpoint回调函数在训练过程中保存最佳模型（以验证精度为标准）进行多个轮次的训练。"
   ],
   "id": "bbeb76c3ffa2e1ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T00:51:41.334181Z",
     "start_time": "2025-10-14T00:34:52.943770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 不用简单RNN,用带有长短记忆的GRU\n",
    "#\n",
    "# [[ 5,  7,  0, ...,  6,  1,  0]]  1 * 100\n",
    "# [[ v5  v7  v0  .... v6  v1  v0]] 1 * 100 * 16\n",
    "\n",
    "# n_tokens:39\n",
    "# embedding： 39 * n 列的矩阵：  5行：  [w0, w1, ... wn-1]\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_shakespeare_model.keras\", monitor=\"val_accuracy\", save_best_only=True\n",
    ")\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=1, callbacks=[model_ckpt])"
   ],
   "id": "ddb8c8daf6234f32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  31247/Unknown \u001B[1m991s\u001B[0m 31ms/step - accuracy: 0.5455 - loss: 1.5034"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31752\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:164: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m31247/31247\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1008s\u001B[0m 32ms/step - accuracy: 0.5745 - loss: 1.3875 - val_accuracy: 0.5335 - val_loss: 1.6007\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T01:17:29.511254Z",
     "start_time": "2025-10-14T01:17:29.507255Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # 训练后搭建最终模型\n",
    "shakespeare_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X-2), # 不使用填充词元(0)  和 未知词元 (1)\n",
    "    model\n",
    "])"
   ],
   "id": "4a9cd3d1b9707e25",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T01:19:35.483092Z",
     "start_time": "2025-10-14T01:19:35.062442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_proba = shakespeare_model.predict(tf.constant([\"To be or not to b\"]))[0, -1]\n",
    "y_proba\n",
    "y_pred = tf.argmax(y_proba)  # 选择概率最高的字母ID\n",
    "\n",
    "print(text_vec_layer.get_vocabulary())\n",
    "\n",
    "text_vec_layer.get_vocabulary()[y_pred + 2]"
   ],
   "id": "660fc80a64c43c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\31752\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 361ms/step\n",
      "['', '[UNK]', ' ', 'e', 't', 'o', 'a', 'i', 'h', 's', 'r', 'n', '\\n', 'l', 'd', 'u', 'm', 'y', 'w', ',', 'c', 'f', 'g', 'b', 'p', ':', 'k', 'v', '.', \"'\", ';', '?', '!', '-', 'j', 'q', 'x', 'z', '3', '&', '$']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 生成莎士比亚文本",
   "id": "97a339785e97f31f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "要使用char-RNN模型生成新文本，可以将一些文本输入模型，让模型预测最有可能的下一个字母，将其添加到文本的末尾，然后将扩展后的文本输入模型来猜测下一个字母，以此类推。这叫作贪婪解码。但是在实践中，这往往导致相同的单词一遍又一遍地重复。\n",
    "\n",
    "相反，可以使用TensorFlow的tf.random.categorical()函数随机采样下一个字符，采样概率等于估计概率。这将生成更多样化和有趣的文本。categorical()函数在给定类别对数概率(logits)的情况下，对随机类别指数进行采样。"
   ],
   "id": "c5d1bcc028d8d3e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T01:19:54.005024Z",
     "start_time": "2025-10-14T01:19:53.976993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "log_probas = tf.math.log([[0.5, 0.4, 0.1]])\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "n = 2000\n",
    "tf.random.categorical(log_probas, num_samples=n)\n",
    "\n"
   ],
   "id": "89b01494486ce3e9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2000), dtype=int64, numpy=array([[0, 1, 0, ..., 1, 0, 2]], dtype=int64)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "为了更好地控制生成文本的多样性，可以将logits（对数概率）除以一个称为“温度”的数字，这个数字可以根据需求进行调整。“温度”接近零将更偏向于高概率字符，而较高“温度”则会使所有字符获得相同的概率。通常在生成相对严谨和精确的文本（例如数学公式）时，较低的“温度”更为适用，而在生成更多样化且有创意的文本时，则适合用较高的“温度”。",
   "id": "aefc5c6f8577915f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T01:23:31.662055Z",
     "start_time": "2025-10-14T01:23:31.658721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def next_char(text, temperature=1):\n",
    "    y_proba = shakespeare_model.predict(tf.constant([text]))[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0,0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]"
   ],
   "id": "b358a7b630203dce",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T01:23:32.064424Z",
     "start_time": "2025-10-14T01:23:32.061092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ],
   "id": "fbd43a4da8c0dd45",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T01:23:34.876379Z",
     "start_time": "2025-10-14T01:23:32.386140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf.random.set_seed(42)\n",
    "print(extend_text(\"To be or not to be\", temperature=1))\n"
   ],
   "id": "786e75ebe35d87e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 73ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 26ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 26ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 25ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 33ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 32ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 26ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 25ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step\n",
      "To be or not to be like a certain.\n",
      "thou art stone, nor wish'd verier\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T01:24:34.008240Z",
     "start_time": "2025-10-14T01:24:31.783913Z"
    }
   },
   "cell_type": "code",
   "source": "print(extend_text(\"To be or not to be\", temperature=1))",
   "id": "ccb5b2f788da0fc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 25ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 20ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "To be or not to been\n",
      "about you have mods you the pascipor our prospe\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T13:48:15.816518Z",
     "start_time": "2025-10-10T13:48:13.225736Z"
    }
   },
   "cell_type": "code",
   "source": "print(extend_text(\"To be or not to be\", temperature=100))",
   "id": "d36fa2f1a2ba4d56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "To be or not to bef ,mt'&ozfpady-$\n",
      "wh!nse?pws3ert--vgerdjw?c-y-ewxnj\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 模拟生成莎士比亚文本的流程 生成名字",
   "id": "c69b7c36365a237d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T02:52:11.756805Z",
     "start_time": "2025-10-14T02:52:11.746372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"datasets/dino.txt\", \"r\") as f:\n",
    "    dino_names = f.read()\n",
    "    dino_names = dino_names.lower()"
   ],
   "id": "8c9db58ee8770353",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T02:52:19.152942Z",
     "start_time": "2025-10-14T02:52:14.312547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\", standardize=\"lower\")\n",
    "\n",
    "text_vec_layer.adapt([dino_names])\n",
    "encoded = text_vec_layer([dino_names])[0]\n",
    "encoded"
   ],
   "id": "82a72b78a106c110",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(19909,), dtype=int64, numpy=array([ 2,  2, 15, ...,  4,  4, 12], dtype=int64)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T02:52:25.272485Z",
     "start_time": "2025-10-14T02:52:25.255088Z"
    }
   },
   "cell_type": "code",
   "source": "text_vec_layer.get_vocabulary()",
   "id": "1febe554f24a9100",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'a',\n",
       " 's',\n",
       " 'u',\n",
       " 'o',\n",
       " 'r',\n",
       " '\\n',\n",
       " 'n',\n",
       " 'i',\n",
       " 'e',\n",
       " 't',\n",
       " 'l',\n",
       " 'p',\n",
       " 'h',\n",
       " 'c',\n",
       " 'g',\n",
       " 'd',\n",
       " 'm',\n",
       " 'y',\n",
       " 'b',\n",
       " 'k',\n",
       " 'v',\n",
       " 'x',\n",
       " 'z',\n",
       " 'j',\n",
       " 'w',\n",
       " 'f',\n",
       " 'q']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T02:52:26.423734Z",
     "start_time": "2025-10-14T02:52:26.405878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoded -= 2\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
    "dataset_size = len(encoded)\n",
    "n_tokens"
   ],
   "id": "340430d9880913e7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T02:52:27.514881Z",
     "start_time": "2025-10-14T02:52:27.495831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "end_of_name_encode = text_vec_layer([\"\\n\"])[0,0].numpy() - 2\n",
    "end_of_name_encode"
   ],
   "id": "8019902ca41e0a37",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T02:52:28.325773Z",
     "start_time": "2025-10-14T02:52:28.229167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 把 Tensor 转成 numpy 数组\n",
    "encoded_np = encoded.numpy()\n",
    "\n",
    "# 存放 (X, Y) 对\n",
    "names_X = []  # [第一个名字不包括换行符， 第二个名字不包括换行符， ...  最后一个名字不包括换行符]\n",
    "names_Y = []  # [第一个名字有换行符，但无第一个字符， .....    , ...  最后一个名字有换行符，但无第一个字符]\n",
    "\n",
    "# 临时缓冲区\n",
    "current_name = []\n",
    "\n",
    "for token in encoded_np:\n",
    "    current_name.append(token)\n",
    "    if token == end_of_name_encode:\n",
    "        if len(current_name) > 1:  # 至少两个字符才能形成 X/Y\n",
    "            X = current_name[:-1]\n",
    "            Y = current_name[1:]\n",
    "            names_X.append(X)\n",
    "            names_Y.append(Y)\n",
    "        current_name = []\n",
    "\n",
    "\n",
    "current_name.append(end_of_name_encode)\n",
    "if len(current_name) > 1:\n",
    "    X = current_name[:-1]\n",
    "    Y = current_name[1:]\n",
    "    names_X.append(X)\n",
    "    names_Y.append(Y)\n",
    "    current_name = []\n",
    "\n",
    "names_X[:5], names_Y[:5]\n",
    "# 转成 TensorFlow 张量\n",
    "X_dataset = tf.ragged.constant(names_X)\n",
    "X_dataset\n",
    "Y_dataset = tf.ragged.constant(names_Y)\n",
    "Y_dataset\n",
    "\n",
    "train_set = tf.data.Dataset.from_tensor_slices((X_dataset, Y_dataset)).shuffle(1000).batch(8)\n",
    "print(\"共提取名字数:\", len(names_X))\n"
   ],
   "id": "f5b11ea33082d4a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共提取名字数: 1536\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T02:52:51.063624Z",
     "start_time": "2025-10-14T02:52:29.305953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.SimpleRNN(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# history = model.fit(X_dataset, Y_dataset, batch_size=1, epochs=10)\n",
    "model.fit(train_set, epochs=20)"
   ],
   "id": "e96aa6e901b201d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Users\\31752\\.conda\\envs\\homl3\\lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_map_ops.py:156: calling map_fn (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "192/192 [==============================] - 3s 5ms/step - loss: 2.4201 - accuracy: 0.3095\n",
      "Epoch 2/20\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.9634 - accuracy: 0.4251\n",
      "Epoch 3/20\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.8499 - accuracy: 0.4477\n",
      "Epoch 4/20\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.7907 - accuracy: 0.4607\n",
      "Epoch 5/20\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.7383 - accuracy: 0.4744\n",
      "Epoch 6/20\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.7000 - accuracy: 0.4894\n",
      "Epoch 7/20\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.6651 - accuracy: 0.4938\n",
      "Epoch 8/20\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.6380 - accuracy: 0.5030\n",
      "Epoch 9/20\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.6126 - accuracy: 0.5140\n",
      "Epoch 10/20\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.5873 - accuracy: 0.5225\n",
      "Epoch 11/20\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.5665 - accuracy: 0.5278\n",
      "Epoch 12/20\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.5424 - accuracy: 0.5365\n",
      "Epoch 13/20\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.5225 - accuracy: 0.5426\n",
      "Epoch 14/20\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 1.5067 - accuracy: 0.5463\n",
      "Epoch 15/20\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.4795 - accuracy: 0.5570\n",
      "Epoch 16/20\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.4647 - accuracy: 0.5589\n",
      "Epoch 17/20\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.4432 - accuracy: 0.5630\n",
      "Epoch 18/20\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.4221 - accuracy: 0.5708\n",
      "Epoch 19/20\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.4037 - accuracy: 0.5733\n",
      "Epoch 20/20\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 1.3840 - accuracy: 0.5798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1460c14dbd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T02:53:10.265176Z",
     "start_time": "2025-10-14T02:53:10.174041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dino_name_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X -2),\n",
    "    model\n",
    "])"
   ],
   "id": "c4e238724c5a6c7",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T02:53:23.112331Z",
     "start_time": "2025-10-14T02:53:23.098451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def my_next_char(model, text, temperature=1):\n",
    "    y_proba = model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0,0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]"
   ],
   "id": "521b1157784271da",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T02:54:10.993346Z",
     "start_time": "2025-10-14T02:54:10.983520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def my_extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        char_gen_next = my_next_char(dino_name_model, text, temperature)\n",
    "        if char_gen_next == \"\\n\":\n",
    "            break\n",
    "        text += my_next_char(dino_name_model, text, temperature)\n",
    "    return text"
   ],
   "id": "2f3eb39386eea6e6",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T02:56:17.450480Z",
     "start_time": "2025-10-14T02:56:16.026651Z"
    }
   },
   "cell_type": "code",
   "source": "my_extend_text(\"xx\")",
   "id": "3c660e0f433513ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'xxaveisaurus'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "46d9c3eed421a220"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl3",
   "language": "python",
   "name": "homl3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

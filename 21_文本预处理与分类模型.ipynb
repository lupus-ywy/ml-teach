{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 自然语言处理（NLP）简介\n",
    "- 预处理文本输入，将其转换为数值输入\n",
    "- 构建简单的文本分类模型，为更复杂的Transformer模型做好准备"
   ],
   "id": "caeaf79ccdb7017e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 准备文本数据",
   "id": "84b5f7ce3071826f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "文本数据的障碍：输入并非数值型数据，需要将文字转换为数字张量，文本的数值可以通过多种方式构建：\n",
    "\n",
    "1. 将输入内容分割成一系列字符，并为每个字符分配一个唯一的索引\n",
    "2. 构建基于单词的表示：首先将句子按空格和标点符号拆分，然后将每个单词映射到唯一的数字表示\n",
    "\n",
    "两种都有用，所有文本预处理都会包含一个分割步骤，即将文本分割成称为\"词元\"的小单元。"
   ],
   "id": "864b99141e5ff94f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 字母和词的分词",
   "id": "8012ed0f759dfd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:48:37.966368Z",
     "start_time": "2025-11-14T11:48:37.918627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import regex as re\n",
    "\n",
    "def split_chars(text):\n",
    "    return re.findall(r\".\", text)"
   ],
   "id": "c3c5b37cb47949e8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:48:42.408024Z",
     "start_time": "2025-11-14T11:48:42.389024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chars = split_chars(\"The quick brown fox jumps over the lazy dog\")\n",
    "chars[:12]"
   ],
   "id": "128abdc946dae25c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T', 'h', 'e', ' ', 'q', 'u', 'i', 'c', 'k', ' ', 'b', 'r']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:48:43.014356Z",
     "start_time": "2025-11-14T11:48:43.003355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# [\\w]+\" 正则表达式会匹配连续的非空白字符，并且 \"[.,!?;]\"能够匹配括号内的标点符号。可以将两者结合起来，得到一个能够将每个单词和标点符号拆分成一个标记的正则表达式：\n",
    "\n",
    "def split_words(text):\n",
    "    return re.findall(r\"[\\w]+|[.,!?;]\", text)"
   ],
   "id": "7c0bd6b9836069aa",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:48:43.546173Z",
     "start_time": "2025-11-14T11:48:43.537136Z"
    }
   },
   "cell_type": "code",
   "source": "split_words(\"The quick brown fox jumped over the dog.\")",
   "id": "df29b9b94068ff1c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'dog', '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "分词（splitting）将从一个完整的字符串转换为一个**词元序列（token sequence）**，但仍然需要把这些字符串词元转换成**数值输入（numeric inputs）**。\n",
    "目前最常见的方法是为每个词元分配一个**唯一的整数索引（integer index）**，这种过程通常称为**输入索引化（indexing our input）**。\n",
    "\n",
    "这种表示方法具有灵活且可逆的特性，能广泛应用于各种建模方式。\n",
    "之后，可以再决定如何将这些**词元索引（token indices）** 映射到模型可接收的 **潜在空间（latent space）** 中。\n",
    "\n",
    "对于字符级token，可以使用ASCII查询来建立索引——例如，ord('A')→65、ord('z')→122。但这种方法在涉及非英文时扩展性较差——Unicode标准包含超过一百万个字符！更稳健的技术是基于训练数据中的特定token构建索引映射，这种在自然语言处理中称为\"词汇表\"的技术，其优势在于能同等适用于单词级和字符级token的处理。"
   ],
   "id": "8350daaad62aea94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:48:44.600449Z",
     "start_time": "2025-11-14T11:48:44.585940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocabulary = {\n",
    "    \"[UNK]\": 0,\n",
    "    \"the\": 1,\n",
    "    \"quick\": 2,\n",
    "    \"brown\": 3,\n",
    "    \"fox\": 4,\n",
    "    \"jumped\": 5,\n",
    "    \"over\": 6,\n",
    "    \"dog\": 7,\n",
    "    \".\": 8,\n",
    "}\n",
    "words = split_words(\"The quick brown fox jumped over the lazy dog.\")\n",
    "indices = [vocabulary.get(word, 0) for word in words]\n",
    "\n",
    "indices"
   ],
   "id": "2e3a4628e88827b2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 3, 4, 5, 6, 1, 0, 7, 8]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "UNK： 代表一个词汇表未知的token，处理所有输入，即使这个token只出现了测试集\n",
    "\n",
    "很多句子非常相似（几乎一样），只有字母大小写，有没有引号，缩写区别或者简体繁体的区别，但是模型并不知道他们没有什么区别，所以需要对文本做标准化避免几乎相同的句子编码差异。不要让模型去处理相同句子的不同表示\n",
    "\n",
    "标准化操作：转小写，去标点符号，繁体转简体\n",
    "\n",
    "文本预处理的三个阶段：\n",
    "1. **Standardization（标准化）** —— 对输入文本进行基本的文本到文本的规范化转换。\n",
    "2. **Splitting（分词）** —— 将文本切分为由词元（token）组成的序列。\n",
    "3. **Indexing（索引化）** —— 使用词汇表（vocabulary）将词元映射为索引。\n",
    "\n",
    "\n",
    "文本 -> 标准化文本 -> 词元 -> 词元索引\n",
    "\n",
    "人们通常把整个过程称为 tokenization（词元化），而把“将文本映射为词元索引序列的对象”称为 tokenizer（分词器）。\n",
    "\n",
    "![文本预处理](./images/text_preprocess.png)"
   ],
   "id": "5808a7ac3fb3ea54"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:48:45.613877Z",
     "start_time": "2025-11-14T11:48:45.601878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# character-level 分词器\n",
    "class CharTokenizer:\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.unk_id = vocabulary[\"[UNK]\"]\n",
    "\n",
    "    def standardize(self, inputs):\n",
    "        return inputs.lower()\n",
    "\n",
    "    def split(self, inputs):\n",
    "        return re.findall(r\".\", inputs)\n",
    "\n",
    "    def index(self, tokens):\n",
    "        return [self.vocabulary.get(t, self.unk_id) for t in tokens]\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        inputs = self.standardize(inputs)\n",
    "        tokens = self.split(inputs)\n",
    "        indices = self.index(tokens)\n",
    "        return indices\n"
   ],
   "id": "ef54455e21263960",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在使用这一方法之前，还需要编写一个函数，用于根据输入文本**构建词元（token）词表**。\n",
    "\n",
    "与其简单地把所有字符都映射到唯一的索引上，更希望能**限制词表的大小**，只保留输入数据中**最常见的词元**。\n",
    "\n",
    "当进入模型构建阶段时，**限制词表大小**将成为**控制模型参数数量**的重要手段。\n"
   ],
   "id": "b3f641bfa8515896"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:48:46.656195Z",
     "start_time": "2025-11-14T11:48:46.637194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import collections\n",
    "\n",
    "def compute_char_vocabulary(inputs, max_size):\n",
    "    char_counts = collections.Counter()\n",
    "    for x in inputs:\n",
    "        x = x.lower()\n",
    "        tokens = re.findall(r\".\", x)\n",
    "        char_counts.update(tokens)\n",
    "\n",
    "    vocabulary = [\"[UNK]\"]\n",
    "    most_common = char_counts.most_common(max_size - len(vocabulary))\n",
    "    for token, count in most_common:\n",
    "        vocabulary.append(token)\n",
    "    return dict((token, i) for i, token in enumerate(vocabulary))"
   ],
   "id": "e3db1e79eebd3cf2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:48:47.140309Z",
     "start_time": "2025-11-14T11:48:47.120978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 建立一个 word-level 分词器（tokenizer) , 只需要改分割的规则\n",
    "class WordTokenizer:\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.unk_id = vocabulary[\"[UNK]\"]\n",
    "\n",
    "    def standardize(self, inputs):\n",
    "        return inputs.lower()\n",
    "\n",
    "    def split(self, inputs):\n",
    "        return re.findall(r\"[\\w]+|[.,!?;]\", inputs)\n",
    "\n",
    "    def index(self, tokens):\n",
    "        return [self.vocabulary.get(t, self.unk_id) for t in tokens]\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        inputs = self.standardize(inputs)\n",
    "        tokens = self.split(inputs)\n",
    "        indices = self.index(tokens)\n",
    "        return indices\n"
   ],
   "id": "bce4fe08230d8dcc",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:48:47.617617Z",
     "start_time": "2025-11-14T11:48:47.611175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 换一个新的分割规则，变成计算word-level的词汇表\n",
    "def compute_word_vocabulary(inputs, max_size):\n",
    "    word_counts = collections.Counter()\n",
    "    for x in inputs:\n",
    "        x = x.lower()\n",
    "        tokens = re.findall(r\"[\\w]+|[.,!?;]\", x)\n",
    "        word_counts.update(tokens)\n",
    "\n",
    "    vocabulary = [\"[UNK]\"]\n",
    "    most_common = word_counts.most_common(max_size - len(vocabulary))\n",
    "    for token, count in most_common:\n",
    "        vocabulary.append(token)\n",
    "    return dict((token, i) for i, token in enumerate(vocabulary))\n"
   ],
   "id": "c097bd7dd77a5072",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![keras和它的后端](./images/RNN/keras.png)",
   "id": "a2eed052b1796fe3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:48:52.184058Z",
     "start_time": "2025-11-14T11:48:48.668622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ],
   "id": "405e47326e952bd8",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:48:52.308706Z",
     "start_time": "2025-11-14T11:48:52.186058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = keras.Sequential([\n",
    "    keras.Input(shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\"),\n",
    "    keras.layers.GlobalAveragePooling2D(),\n",
    "    keras.layers.Dense(units=10)\n",
    "])"
   ],
   "id": "b25fab517649791a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:48:56.536553Z",
     "start_time": "2025-11-14T11:48:56.515849Z"
    }
   },
   "cell_type": "code",
   "source": "model.summary()",
   "id": "33bf31c9df8bd96e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 64)        640       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 13, 13, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 5, 5, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 256)         295168    \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 256)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 372234 (1.42 MB)\n",
      "Trainable params: 372234 (1.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:49:00.405881Z",
     "start_time": "2025-11-14T11:49:00.274331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype(\"float32\") / 255"
   ],
   "id": "22ef0e598b8226d6",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:52:13.723144Z",
     "start_time": "2025-11-14T11:49:03.678864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.compile(optimizer=\"adam\", loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
   ],
   "id": "b6c602102ba71369",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 37s 38ms/step - loss: 0.2626 - accuracy: 0.9183\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 37s 39ms/step - loss: 0.0761 - accuracy: 0.9767\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 39s 41ms/step - loss: 0.0525 - accuracy: 0.9837\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 39s 42ms/step - loss: 0.0378 - accuracy: 0.9881\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 39s 41ms/step - loss: 0.0309 - accuracy: 0.9902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e0ae027850>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:52:33.628144Z",
     "start_time": "2025-11-14T11:52:30.251064Z"
    }
   },
   "cell_type": "code",
   "source": "model.evaluate(test_images, test_labels)",
   "id": "874ca970a4a198bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 10ms/step - loss: 0.0286 - accuracy: 0.9903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.028571555390954018, 0.9902999997138977]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:52:52.550288Z",
     "start_time": "2025-11-14T11:52:52.115528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 自定义分词器 用于现实中的文本输入\n",
    "import keras\n",
    "\n",
    "filename = keras.utils.get_file(\n",
    "    origin=\"https://www.gutenberg.org/files/2701/old/moby10b.txt\",\n",
    ")\n",
    "moby_dick = list(open(filename, \"r\"))\n",
    "\n",
    "vocabulary = compute_char_vocabulary(moby_dick, max_size=100)\n",
    "char_tokenizer = CharTokenizer(vocabulary)\n"
   ],
   "id": "e68e0509314f4ac4",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:52:36.107662Z",
     "start_time": "2025-11-14T11:52:36.089828Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Vocabulary length: \", )",
   "id": "4aeac026f433cf76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: \n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:52:36.666771Z",
     "start_time": "2025-11-14T11:52:36.656673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Vocabulary length:\", len(vocabulary))\n",
    "print(\"Vocabulary start:\", list(vocabulary.keys())[:10])\n",
    "print(\"Vocabulary end:\", list(vocabulary.keys())[-10:])\n",
    "print(\"Line length:\", len(char_tokenizer( \"Call me Ishmael. Some years ago--never mind how long precisely.\")))"
   ],
   "id": "100c59a0910390b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 64\n",
      "Vocabulary start: ['[UNK]', ' ', 'e', 't', 'a', 'o', 'n', 'i', 's', 'h']\n",
      "Vocabulary end: ['@', '$', '%', '#', '=', '~', '&', '+', '<', '>']\n",
      "Line length: 63\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:52:37.487806Z",
     "start_time": "2025-11-14T11:52:37.192842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocabulary = compute_word_vocabulary(moby_dick, max_size=2_000)\n",
    "word_tokenizer = WordTokenizer(vocabulary)"
   ],
   "id": "4f8daaee4660509",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:52:37.798825Z",
     "start_time": "2025-11-14T11:52:37.787018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Vocabulary length:\", len(vocabulary))\n",
    "print(\"Vocabulary start:\", list(vocabulary.keys())[:5])\n",
    "print(\"Vocabulary end:\", list(vocabulary.keys())[-5:])\n",
    "print(\"Line length:\", len(word_tokenizer(\"Call me Ishmael. Some years ago--never mind how long precisely.\")))"
   ],
   "id": "fffb5c7b1188e281",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 2000\n",
      "Vocabulary start: ['[UNK]', ',', 'the', '.', 'of']\n",
      "Vocabulary end: ['tambourine', 'subtle', 'perseus', 'elevated', 'repose']\n",
      "Line length: 13\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "已经可以看到两种分词（tokenization）方法的优缺点。\n",
    "字符级分词器（character-level tokenizer） 只需要 64 个词汇项就能覆盖整本书，但会把每个输入编码成非常长的序列。\n",
    "词级分词器（word-level tokenizer） 很快就能填满一个包含 2000 个词的词表（如果想覆盖整本书中出现的所有词汇，你需要一个包含 17,000 个词的词典！），但词级分词器输出的序列会短得多。\n",
    "\n",
    "随着机器学习实践者使用越来越多的数据和参数来扩大模型规模，词级分词和字符级分词各自的缺陷也逐渐显现。\n",
    "词级分词带来的“压缩”效果实际上非常重要——它能让模型一次读取更长的文本序列。\n",
    "但是，如果你尝试为一个大型数据集（如今的数据集可能包含数万亿个单词）构建词级词表，那么这个词表将会大到难以处理，可能包含上亿个词条。\n",
    "而如果你为了限制词表大小而强行压缩它，那么会导致大量文本被编码成 \"[UNK]\"（未知词符），从而丢失宝贵的信息。\n",
    "\n",
    "这些问题促使一种新的分词方式流行起来——子词分词（subword tokenization），\n",
    "它试图在词级和字符级分词之间找到一个平衡点。"
   ],
   "id": "4629ef9296c4511f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 子词分词",
   "id": "1999a37e1e7a44d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "子词分词（Subword tokenization） 的目标是结合 字符级编码 和 词级编码 的优点。\n",
    "希望既能拥有 词级分词器（WordTokenizer） 那种输出简洁的特性，\n",
    "又能具备 字符级分词器（CharTokenizer） 那种用极小词表就能编码各种输入的能力。\n",
    "\n",
    "可以把寻找理想分词器的过程，看作是在寻找一种理想的数据压缩方式。\n",
    "减少词元（token）的数量，相当于压缩输入序列的总体长度；\n",
    "而使用更小的词表，则能减少表示每个词元所需的存储字节数。\n",
    "如果能同时做到这两点，就可以把短且信息密度高的序列输入到深度学习模型中。\n",
    "\n",
    "压缩与分词之间的这种类比并非一开始就显而易见，但事实证明它非常有用。\n",
    "在过去十年的自然语言处理研究中，最实用、最有效的技巧之一\n",
    "是重新利用 1990 年代的一种无损压缩算法——字节对编码（Byte-Pair Encoding, BPE） 来做分词。\n",
    "这一算法至今仍被 ChatGPT 以及许多其他模型所采用。\n",
    "\n",
    "字节对编码（BPE）的核心思想是：\n",
    "从最基础的字符级词表开始，\n",
    "逐步地将出现频率最高的字符对（pair）合并，形成越来越长的子词序列。（找出在语料中相邻出现最频繁的符号对（pair），逐步合并成更大的子词。）\n",
    "\n",
    "将实现一个使用 BPE 算法 的分词器。"
   ],
   "id": "deb60884188e37a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:52:43.093105Z",
     "start_time": "2025-11-14T11:52:43.088104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = [\n",
    "    \"the quick brown fox\",\n",
    "    \"the slow brown fox\",\n",
    "    \"the quick brown foxhound\",\n",
    "]  # 假设这是一个数据集"
   ],
   "id": "e7109812de579ef7",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "与 WordTokenizer（词级分词器） 类似，首先要统计文本中所有单词的出现次数。\n",
    "在创建单词计数字典的过程中，会把所有文本拆分成单个字符，\n",
    "然后用空格将这些字符连接起来。\n",
    "\n",
    "这样处理的目的是：\n",
    "在接下来的步骤中，就能更方便地统计并合并字符对（character pairs）。"
   ],
   "id": "78eed18eceb38f00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:52:44.316101Z",
     "start_time": "2025-11-14T11:52:44.304557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_and_split_words(data):\n",
    "    counts = collections.Counter()\n",
    "    for line in data:\n",
    "        line = line.lower()\n",
    "        for word in re.findall(r\"[\\w]+|[.,!?;]\", line):\n",
    "            chars = re.findall(r\".\", word)\n",
    "            split_word = \" \".join(chars)\n",
    "            counts[split_word] += 1\n",
    "    return dict(counts)\n",
    "\n",
    "counts = count_and_split_words(data)"
   ],
   "id": "39ca663d0e53f93f",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T11:52:44.941858Z",
     "start_time": "2025-11-14T11:52:44.927604Z"
    }
   },
   "cell_type": "code",
   "source": "counts",
   "id": "a885de51200383a2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t h e': 3,\n",
       " 'q u i c k': 2,\n",
       " 'b r o w n': 3,\n",
       " 'f o x': 2,\n",
       " 's l o w': 1,\n",
       " 'f o x h o u n d': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "为了在拆分后的词频统计上应用 字节对编码（Byte-Pair Encoding, BPE），\n",
    "需要找到一对字符并将它们合并成一个新的符号。\n",
    "\n",
    "具体来说，会在所有单词中遍历所有可能的字符对（character pairs），\n",
    "然后只合并其中出现频率最高的一对。\n",
    "\n",
    "在前面的例子中，最常见的字符对是 (\"o\", \"w\")，\n",
    "它出现在单词 “brown”（在数据中出现了三次）和 “slow”（出现一次）中。\n",
    "因此将这对字符合并为一个新的符号 “ow”，\n",
    "并把所有出现的 “o w” 替换成 “ow”。\n",
    "\n",
    "接着继续重复这个过程：\n",
    "统计新的字符对频率、找到最常见的一对并进行合并。\n",
    "不同的是，现在 “ow” 被视为一个整体符号，\n",
    "它可能会再与 “l” 合并形成 “low”。\n",
    "\n",
    "通过这种不断合并出现频率最高的符号对的方式，\n",
    "逐步构建出一个越来越大的子词词表（subword vocabulary）。"
   ],
   "id": "3d8c3b89efd941fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T14:32:58.878792Z",
     "start_time": "2025-11-11T14:32:58.865761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_pairs(counts):\n",
    "    pairs = collections.Counter()\n",
    "    for word, freq in counts.items():\n",
    "        symbols = word.split()\n",
    "        for pair in zip(symbols[:-1], symbols[1:]):\n",
    "            pairs[pair] += freq\n",
    "\n",
    "    return pairs\n",
    "\n",
    "def merge_pair(counts, first, second):\n",
    "    #  在“空格分隔的词元序列”中，前后都不是字母/数字的地方，找出 完整独立的“{first} {second}” 组合。\n",
    "    split = re.compile(f\"(?<!\\S){first} {second}(?!\\S)\")\n",
    "\n",
    "    # 把所有的组合 换成 合并的\n",
    "    merged = f\"{first}{second}\"\n",
    "    return {split.sub(merged, word): count for word, count in counts.items()}\n",
    "\n",
    "for i in range(10):\n",
    "    pairs = count_pairs(counts)\n",
    "    first, second = max(pairs, key=pairs.get)\n",
    "    counts = merge_pair(counts, first, second)\n",
    "    print(list(counts.keys()))"
   ],
   "id": "646845ff9c0ce674",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t h e', 'q u i c k', 'b r ow n', 'f o x', 's l ow', 'f o x h o u n d']\n",
      "['th e', 'q u i c k', 'b r ow n', 'f o x', 's l ow', 'f o x h o u n d']\n",
      "['the', 'q u i c k', 'b r ow n', 'f o x', 's l ow', 'f o x h o u n d']\n",
      "['the', 'q u i c k', 'br ow n', 'f o x', 's l ow', 'f o x h o u n d']\n",
      "['the', 'q u i c k', 'brow n', 'f o x', 's l ow', 'f o x h o u n d']\n",
      "['the', 'q u i c k', 'brown', 'f o x', 's l ow', 'f o x h o u n d']\n",
      "['the', 'q u i c k', 'brown', 'fo x', 's l ow', 'fo x h o u n d']\n",
      "['the', 'q u i c k', 'brown', 'fox', 's l ow', 'fox h o u n d']\n",
      "['the', 'qu i c k', 'brown', 'fox', 's l ow', 'fox h o u n d']\n",
      "['the', 'qui c k', 'brown', 'fox', 's l ow', 'fox h o u n d']\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "可以看到，高频词通常会被完全合并成一个整体，而低频词则只会被部分合并。\n",
    "\n",
    "接下来，可以把这一过程扩展为一个完整的函数，用于构建 字节对编码（Byte-Pair Encoding, BPE）词表。\n",
    "\n",
    "首先以输入文本中出现的所有字符作为初始词表，\n",
    "然后不断地将出现频率最高的符号对（pair）合并，\n",
    "逐步向词表中加入越来越长的子词（subword），\n",
    "直到词表的大小达到设定的目标。\n",
    "\n",
    "同时，还需要维护一个单独的合并规则字典（merge rules），\n",
    "记录每次合并的符号对及其合并顺序（rank order）。\n",
    "\n",
    "在下一步中，将利用这些合并规则来对新的输入文本进行分词。"
   ],
   "id": "12a62216b2272928"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T14:46:03.903916Z",
     "start_time": "2025-11-11T14:46:03.891397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_sub_word_vocabulary(dataset, vocab_size):\n",
    "    counts = count_and_split_words(dataset)\n",
    "\n",
    "    char_counts = collections.Counter()\n",
    "    for word in counts:\n",
    "        for char in word.split():\n",
    "            char_counts[char] += counts[word]\n",
    "\n",
    "    most_common = char_counts.most_common()\n",
    "    vocab = [\"[UNK]\"] + [char for char, freq in most_common]\n",
    "    merges = []\n",
    "\n",
    "    while len(vocab) < vocab_size:\n",
    "        pairs = count_pairs(counts)\n",
    "        if not pairs:\n",
    "            break\n",
    "        first, second = max(pairs, key=pairs.get)\n",
    "        counts = merge_pair(counts, first, second)\n",
    "        vocab.append(f\"{first}{second}\")\n",
    "        merges.append(f\"{first} {second}\")\n",
    "\n",
    "    vocab = dict((token, index) for index, token in enumerate(vocab))\n",
    "    merges = dict((token, rank) for rank, token in enumerate(merges))\n",
    "\n",
    "    return vocab, merges"
   ],
   "id": "1c68ad96ac6b6d13",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "让来构建一个 SubWordTokenizer（子词分词器），它能够根据学到的 **合并规则（merge rules)** 来对新的输入文本进行分词。\n",
    "\n",
    "其中，standardize() 和 index() 两个步骤可以与 WordTokenizer（词级分词器） 保持一致，所有的变化都将发生在 split() 方法 中。\n",
    "\n",
    "在分词步骤中，首先将输入文本按空格切分成单词，然后再将每个单词拆分成字符，接着应用训练得到的合并规则，逐步将相邻字符合并。\n",
    "\n",
    "经过这些合并后，剩下的就是子词（subwords） ——这些子词可能是完整的单词、单词的一部分，也可能只是单个字符，具体取决于该单词在训练数据中的出现频率。\n",
    "\n",
    "最终，这些子词（subwords）就构成了的输出词元（tokens）。"
   ],
   "id": "81b74fa6efc91165"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T16:03:54.747341Z",
     "start_time": "2025-11-11T16:03:54.714089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SubWordTokenizer:\n",
    "    def __init__(self, vocabulary, merges):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.merges = merges\n",
    "        self.unk_id = vocabulary[\"[UNK]\"]\n",
    "\n",
    "    def standardize(self, inputs):\n",
    "        return inputs.lower()\n",
    "\n",
    "    def bpe_merge(self, word):\n",
    "        while True:\n",
    "            # 匹配word里所有的符号对\n",
    "            pairs = re.findall(r\"(?<!\\S)\\S+ \\S+(?!\\S)\", word, overlapped=True)\n",
    "            if not pairs:\n",
    "                break\n",
    "\n",
    "            # 通过rank 执行merge规则，最频繁的 符号对最先合并\n",
    "            best = min(pairs, key=lambda pair: self.merges.get(pair, 1e9))\n",
    "            if best not in self.merges:\n",
    "                break\n",
    "\n",
    "            first, second = best.split()\n",
    "            split = re.compile(f\"(?<!\\S){first} {second}(?!\\S)\")\n",
    "            merged = f\"{first}{second}\"\n",
    "            word = split.sub(merged, word)\n",
    "        return word\n",
    "\n",
    "\n",
    "    def split(self, inputs):\n",
    "        tokens = []\n",
    "        # 切分单词\n",
    "        for word in re.findall(r\"[\\w]+|[.,!?;]\", inputs):\n",
    "            # 通过空格连接字母\n",
    "            word = \" \".join(re.findall(r\".\", word))\n",
    "\n",
    "            # 应用BPE编码规则\n",
    "            word = self.bpe_merge(word)\n",
    "            tokens.extend(word.split())\n",
    "        return tokens\n",
    "\n",
    "    def index(self, tokens):\n",
    "        return [self.vocabulary.get(t, self.unk_id) for t in tokens]\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        inputs = self.standardize(inputs)\n",
    "        tokens = self.split(inputs)\n",
    "        indices = self.index(tokens)\n",
    "        return indices"
   ],
   "id": "93ed2ee1dc7bbcff",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T16:05:01.534116Z",
     "start_time": "2025-11-11T16:04:02.485875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocabulary, merges = compute_sub_word_vocabulary(moby_dick, 2_000)\n",
    "sub_word_tokenizer = SubWordTokenizer(vocabulary, merges)"
   ],
   "id": "a67123b2ed3d38b3",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T16:05:40.332459Z",
     "start_time": "2025-11-11T16:05:40.302921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Vocabulary length:\", len(vocabulary))\n",
    "print(\"Vocabulary start:\", list(vocabulary.keys())[:10])\n",
    "print(\"Vocabulary end:\", list(vocabulary.keys())[-7:])\n",
    "print(\"Line length:\", len(sub_word_tokenizer(\"Call me Ishmael. Some years ago--never mind how long precisely.\"\n",
    ")))"
   ],
   "id": "3cf1135ce4dd6f7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 2000\n",
      "Vocabulary start: ['[UNK]', 'e', 't', 'a', 'o', 'n', 'i', 's', 'h', 'r']\n",
      "Vocabulary end: ['bright', 'pilot', 'sco', 'ben', 'dem', 'gale', 'ilo']\n",
      "Line length: 16\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "对于的测试句子来说，SubWordTokenizer（子词分词器） 的分词结果稍微比 WordTokenizer（词级分词器） 长一些（16 个词元 vs 13 个词元），\n",
    "但与 WordTokenizer 不同的是，SubWordTokenizer 能够对《白鲸记》（Moby Dick）中的每一个单词进行分词，而无需使用 \"[UNK]\"（未知词）标记。SubWordTokenizer 不需要 [UNK]，因为它的词表覆盖了所有字符（字符级基础），任何新词都能被拆分成已知的子词或字符组合。\n",
    "\n",
    "因为词表中包含了源文本中的所有字符，所以在最糟糕的情况下，模型也只是把一个单词拆成单个字符来处理。换句话说，在保持较短平均词元长度的同时，又能用一个较小的词表有效地处理罕见词。 这正是 **子词分词器（subword tokenizer**的优势所在。\n",
    "\n",
    "你可能会注意到，运行这段代码的速度要明显慢于字符级或词级分词器——大约需要一分钟左右。这是因为学习合并规则（merge rules）的过程远比简单地统计输入数据集中的单词频率复杂得多。虽然这是子词分词的一项缺点，但在实际应用中，这通常并不是什么严重问题：一个模型只需要学习一次词表，\n",
    "而子词词表学习的开销，相比于模型训练成本，几乎可以忽略不计。\n",
    "\n",
    "已经看到了三种不同的文本分词方法。现在，已经能够把文本转换为数值输入，接下来就可以进入模型训练阶段了。\n",
    "\n",
    "虽然理解分词器的工作原理非常重要，但在实际项目中，几乎不需要自己从零实现一个分词器。Keras（以及大多数深度学习框架）都提供了现成的文本分词工具。"
   ],
   "id": "bd99781f2acefac7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**应该使用哪种分词技术？**\n",
    "\n",
    "在处理一个新的文本建模问题时，首先需要回答的问题之一就是：\n",
    "\n",
    "> **该如何对输入文本进行分词（tokenization）？**\n",
    "\n",
    "如果使用的是**预训练模型（pretrained model）**，这个问题其实是**没有选择的**：\n",
    "你必须严格遵循模型在预训练阶段所使用的**完全相同的分词方式**，\n",
    "否则就会破坏模型权重中对输入词元所学习到的有用表示。\n",
    "\n",
    "\n",
    "\n",
    "如果是**从零开始训练模型**，则可以根据任务的特点灵活选择分词方式。\n",
    "一般来说，**词级（word-level）** 或 **子词级（subword-level）** 分词所带来的“压缩效果”极其重要，不可忽视。\n",
    "输入序列平均长度越短，模型就越容易捕捉文本中的**长距离依赖关系（long-range dependencies）**，\n",
    "从而提升整体性能。\n",
    "这正是为什么 **子词分词（subword tokenization）** 成为现代语言模型的主流选择：\n",
    "它既能处理罕见词或拼写错误的单词，\n",
    "又不会让常见词的 token 数量暴涨。\n",
    "\n",
    "\n",
    "\n",
    "不过，**没有一种分词方式是万能的**。\n",
    "在某些 NLP 任务中（例如**拼写纠错**），\n",
    "使用**字符级分词（character-level tokenization）** 可能更有优势，\n",
    "因为它能捕捉到更底层的细节。\n",
    "\n",
    "相反，**词级分词（word-level tokenization）** 的优势在于简单直观：\n",
    "每个模型输入都对应一个人类可读的单词。\n",
    "这不仅易于理解，还使得分析“哪些 token 对预测结果最重要”更加直观明了。\n",
    "\n"
   ],
   "id": "6230720ce294b6c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 文本分类",
   "id": "f7a066a7cd7ab2e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:41.449224Z",
     "start_time": "2025-11-14T14:25:41.432195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# todo： 有顺序的词元 和 没有顺序的词元\n",
    "import keras"
   ],
   "id": "39fe32b75086b6c1",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:25:48.144067Z",
     "start_time": "2025-11-14T14:25:48.126708Z"
    }
   },
   "cell_type": "code",
   "source": "keras.__version__",
   "id": "b29182601c6b852e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.14.0'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:59:30.215159Z",
     "start_time": "2025-11-14T14:57:45.471990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "import os\n",
    "from keras.utils import get_file\n",
    "\n",
    "# 删除已损坏的文件\n",
    "cache_dir = os.path.join(os.path.expanduser('~'), '.keras', 'datasets')\n",
    "imdb_files = [\n",
    "    os.path.join(cache_dir, 'imdb'),\n",
    "    os.path.join(cache_dir, 'imdb.tar.gz')\n",
    "]\n",
    "\n",
    "for file_path in imdb_files:\n",
    "    if os.path.exists(file_path):\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        else:\n",
    "            import shutil\n",
    "            shutil.rmtree(file_path)\n",
    "\n",
    "# 重新下载\n",
    "zip_path = get_file(\n",
    "    origin=\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
    "    fname=\"imdb\",\n",
    "    extract=True,\n",
    ")\n"
   ],
   "id": "d8cfb22b6a537f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84125825/84125825 [==============================] - 12s 0us/step\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T15:15:35.581309Z",
     "start_time": "2025-11-14T15:15:35.571715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "imdb_extract_dir = pathlib.Path(zip_path) / \"..\" / \"aclImdb\"\n",
    "print(imdb_extract_dir)"
   ],
   "id": "cd728f2ec5ae7626",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31752\\.keras\\datasets\\imdb\\..\\aclImdb\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T15:15:38.387001Z",
     "start_time": "2025-11-14T15:15:38.367362Z"
    }
   },
   "cell_type": "code",
   "source": "print(open(imdb_extract_dir / \"train\" / \"pos\" / \"4077_10.txt\", \"r\").read())",
   "id": "162eafe0b360eb2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T15:17:12.727135Z",
     "start_time": "2025-11-14T15:17:12.716692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for path in imdb_extract_dir.glob(\"*/*\"):\n",
    "    if path.is_dir():\n",
    "        print(path)"
   ],
   "id": "fd407369884dcf08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31752\\.keras\\datasets\\imdb\\..\\aclImdb\\test\\neg\n",
      "C:\\Users\\31752\\.keras\\datasets\\imdb\\..\\aclImdb\\test\\pos\n",
      "C:\\Users\\31752\\.keras\\datasets\\imdb\\..\\aclImdb\\train\\neg\n",
      "C:\\Users\\31752\\.keras\\datasets\\imdb\\..\\aclImdb\\train\\pos\n",
      "C:\\Users\\31752\\.keras\\datasets\\imdb\\..\\aclImdb\\train\\unsup\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T16:04:07.414339Z",
     "start_time": "2025-11-14T15:56:45.203363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Splits the training data into a train set and a validation set\n",
    "train_dir = pathlib.Path(\"imdb_train\")\n",
    "test_dir = pathlib.Path(\"imdb_test\")\n",
    "val_dir = pathlib.Path(\"imdb_val\")\n",
    "\n",
    "# Moves the test data unaltered\n",
    "shutil.copytree(imdb_extract_dir / \"test\", test_dir)\n",
    "\n",
    "# train valid split\n",
    "val_percentage = 0.2\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    src_dir = imdb_extract_dir / \"train\" / category\n",
    "    src_files = os.listdir(src_dir)\n",
    "    random.Random(1337).shuffle(src_files)\n",
    "    num_val_samples = int(len(src_files) * val_percentage)\n",
    "\n",
    "    os.makedirs(val_dir / category)\n",
    "    for file in src_files[:num_val_samples]:\n",
    "        shutil.copy(src_dir / file, val_dir / category / file)\n",
    "    os.makedirs(train_dir / category)\n",
    "    for file in src_files[num_val_samples:]:\n",
    "        shutil.copy(src_dir / file, train_dir / category / file)"
   ],
   "id": "53d530844c9b89d3",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T16:05:41.262898Z",
     "start_time": "2025-11-14T16:05:38.666940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.utils import text_dataset_from_directory\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = text_dataset_from_directory(train_dir, batch_size=batch_size)\n",
    "val_ds = text_dataset_from_directory(val_dir, batch_size=batch_size)\n",
    "test_ds = text_dataset_from_directory(test_dir, batch_size=batch_size)\n"
   ],
   "id": "b4190622ffa1af21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### bag of words",
   "id": "d548976a6c920143"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T16:06:13.996575Z",
     "start_time": "2025-11-14T16:06:09.272648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras import layers\n",
    "\n",
    "max_tokens = 20_000\n",
    "\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    split='whitespace',\n",
    "    output_mode='multi_hot',\n",
    ")\n",
    "\n",
    "train_ds_no_labels = train_ds.map(lambda x,y: x)\n",
    "text_vectorization.adapt(train_ds_no_labels)\n",
    "\n",
    "bag_of_words_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y)\n",
    ")\n",
    "bag_of_words_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y)\n",
    ")\n",
    "bag_of_words_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y)\n",
    ")"
   ],
   "id": "eea6d0b04f29f09f",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T16:06:14.106510Z",
     "start_time": "2025-11-14T16:06:13.997572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x, y = next(bag_of_words_train_ds.as_numpy_iterator())\n",
    "x.shape\n",
    "y.shape\n"
   ],
   "id": "14d75b5be5c494",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T16:11:01.759973Z",
     "start_time": "2025-11-14T16:11:01.708760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_linear_classifier(max_tokens, name):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(inputs)\n",
    "    model = keras.Model(inputs, outputs, name=name)\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_linear_classifier(max_tokens, \"bag_of_words_classifier\")"
   ],
   "id": "9e9073b85bb88846",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T16:11:02.441121Z",
     "start_time": "2025-11-14T16:11:02.429621Z"
    }
   },
   "cell_type": "code",
   "source": "model.summary()",
   "id": "d918749f996c9b34",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bag_of_words_classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 20001     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20001 (78.13 KB)\n",
      "Trainable params: 20001 (78.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T16:11:53.530662Z",
     "start_time": "2025-11-14T16:11:03.769623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    restore_best_weights=True,\n",
    "    patience=2,\n",
    ")\n",
    "history = model.fit(\n",
    "    bag_of_words_train_ds,\n",
    "    validation_data=bag_of_words_val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ],
   "id": "b3091704938f0bf6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.4611 - accuracy: 0.8428 - val_loss: 0.3605 - val_accuracy: 0.8810\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 5s 9ms/step - loss: 0.2944 - accuracy: 0.9081 - val_loss: 0.3061 - val_accuracy: 0.8908\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2365 - accuracy: 0.9294 - val_loss: 0.2839 - val_accuracy: 0.8942\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.2012 - accuracy: 0.9441 - val_loss: 0.2724 - val_accuracy: 0.8948\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 5s 9ms/step - loss: 0.1755 - accuracy: 0.9530 - val_loss: 0.2663 - val_accuracy: 0.8946\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 5s 9ms/step - loss: 0.1555 - accuracy: 0.9607 - val_loss: 0.2635 - val_accuracy: 0.8958\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 5s 9ms/step - loss: 0.1393 - accuracy: 0.9669 - val_loss: 0.2621 - val_accuracy: 0.8954\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 5s 9ms/step - loss: 0.1255 - accuracy: 0.9718 - val_loss: 0.2629 - val_accuracy: 0.8942\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 5s 9ms/step - loss: 0.1137 - accuracy: 0.9757 - val_loss: 0.2635 - val_accuracy: 0.8948\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T16:14:11.909617Z",
     "start_time": "2025-11-14T16:14:10.695459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "plt.plot(epochs, accuracy, \"r--\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "5ec7862d5cfd14e9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T16:14:18.394438Z",
     "start_time": "2025-11-14T16:14:13.010463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loss, test_acc = model.evaluate(bag_of_words_test_ds)\n",
    "test_acc"
   ],
   "id": "1f0bc76eba9c3c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 5s 7ms/step - loss: 0.2813 - accuracy: 0.8864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8864399790763855"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### bigram model",
   "id": "6f9f87e324b908d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:17:00.690681Z",
     "start_time": "2025-11-13T13:16:50.352257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_tokens = 30_000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    # Learns a word-level vocabulary\n",
    "    split=\"whitespace\",\n",
    "    output_mode=\"multi_hot\",\n",
    "    # Considers all unigrams and bigrams\n",
    "    ngrams=2,\n",
    ")\n",
    "text_vectorization.adapt(train_ds_no_labels)"
   ],
   "id": "9e2ad89a2086914b",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:17:02.107903Z",
     "start_time": "2025-11-13T13:17:01.999903Z"
    }
   },
   "cell_type": "code",
   "source": "text_vectorization.get_vocabulary()",
   "id": "9d9c344c6ba4e628",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]',\n",
       " 'the',\n",
       " 'and',\n",
       " 'a',\n",
       " 'of',\n",
       " 'to',\n",
       " 'is',\n",
       " 'in',\n",
       " 'it',\n",
       " 'i',\n",
       " 'this',\n",
       " 'that',\n",
       " 'br',\n",
       " 'was',\n",
       " 'as',\n",
       " 'with',\n",
       " 'for',\n",
       " 'movie',\n",
       " 'but',\n",
       " 'of the',\n",
       " 'film',\n",
       " 'on',\n",
       " 'not',\n",
       " 'you',\n",
       " 'his',\n",
       " 'are',\n",
       " 'have',\n",
       " 'he',\n",
       " 'be',\n",
       " 'one',\n",
       " 'in the',\n",
       " 'its',\n",
       " 'at',\n",
       " 'all',\n",
       " 'by',\n",
       " 'an',\n",
       " 'they',\n",
       " 'from',\n",
       " 'who',\n",
       " 'so',\n",
       " 'like',\n",
       " 'her',\n",
       " 'just',\n",
       " 'or',\n",
       " 'about',\n",
       " 'has',\n",
       " 'if',\n",
       " 'out',\n",
       " 'some',\n",
       " 'what',\n",
       " 'there',\n",
       " 'this movie',\n",
       " 'good',\n",
       " 'when',\n",
       " 'more',\n",
       " 'very',\n",
       " 'and the',\n",
       " 'is a',\n",
       " 'my',\n",
       " 'she',\n",
       " 'even',\n",
       " 'no',\n",
       " 'up',\n",
       " 'the film',\n",
       " 'would',\n",
       " 'to the',\n",
       " 'which',\n",
       " 'time',\n",
       " 'only',\n",
       " 'to be',\n",
       " 'really',\n",
       " 'their',\n",
       " 'story',\n",
       " 'see',\n",
       " 'had',\n",
       " 'were',\n",
       " 'the movie',\n",
       " 'can',\n",
       " 'me',\n",
       " 'this film',\n",
       " 'than',\n",
       " 'it is',\n",
       " 'we',\n",
       " 'much',\n",
       " 'well',\n",
       " 'get',\n",
       " 'this is',\n",
       " 'been',\n",
       " 'will',\n",
       " 'into',\n",
       " 'also',\n",
       " 'on the',\n",
       " 'people',\n",
       " 'bad',\n",
       " 'do',\n",
       " 'other',\n",
       " 'in a',\n",
       " 'great',\n",
       " 'how',\n",
       " 'because',\n",
       " 'first',\n",
       " 'most',\n",
       " 'him',\n",
       " 'dont',\n",
       " 'it was',\n",
       " 'then',\n",
       " 'one of',\n",
       " 'for the',\n",
       " 'made',\n",
       " 'with the',\n",
       " 'movies',\n",
       " 'way',\n",
       " 'make',\n",
       " 'of a',\n",
       " 'films',\n",
       " 'could',\n",
       " 'br the',\n",
       " 'them',\n",
       " 'any',\n",
       " 'too',\n",
       " 'after',\n",
       " 'characters',\n",
       " 'think',\n",
       " 'is the',\n",
       " 'at the',\n",
       " 'as a',\n",
       " 'watch',\n",
       " 'being',\n",
       " 'seen',\n",
       " 'two',\n",
       " 'many',\n",
       " 'character',\n",
       " 'never',\n",
       " 'little',\n",
       " 'best',\n",
       " 'plot',\n",
       " 'acting',\n",
       " 'br br',\n",
       " 'where',\n",
       " 'love',\n",
       " 'did',\n",
       " 'in this',\n",
       " 'from the',\n",
       " 'life',\n",
       " 'know',\n",
       " 'with a',\n",
       " 'show',\n",
       " 'as the',\n",
       " 'does',\n",
       " 'ever',\n",
       " 'if you',\n",
       " 'your',\n",
       " 'still',\n",
       " 'better',\n",
       " 'over',\n",
       " 'these',\n",
       " 'off',\n",
       " 'say',\n",
       " 'end',\n",
       " 'i was',\n",
       " 'while',\n",
       " 'scene',\n",
       " 'here',\n",
       " 'man',\n",
       " 'why',\n",
       " 'to see',\n",
       " 'that the',\n",
       " 'the story',\n",
       " 'out of',\n",
       " 'scenes',\n",
       " 'such',\n",
       " 'something',\n",
       " 'go',\n",
       " 'should',\n",
       " 'by the',\n",
       " 'im',\n",
       " 'through',\n",
       " 'back',\n",
       " 'those',\n",
       " 'and i',\n",
       " 'the first',\n",
       " 'was a',\n",
       " 'movie is',\n",
       " 'doesnt',\n",
       " 'watching',\n",
       " 'i have',\n",
       " 'real',\n",
       " 'for a',\n",
       " 'years',\n",
       " 'actors',\n",
       " 'all the',\n",
       " 'thing',\n",
       " 'though',\n",
       " 'there is',\n",
       " 'now',\n",
       " 'didnt',\n",
       " 'new',\n",
       " 'have been',\n",
       " 'makes',\n",
       " 'actually',\n",
       " 'nothing',\n",
       " 'of this',\n",
       " 'another',\n",
       " 'before',\n",
       " 'work',\n",
       " 'find',\n",
       " 'and a',\n",
       " 'film is',\n",
       " 'old',\n",
       " 'funny',\n",
       " 'same',\n",
       " 'going',\n",
       " 'look',\n",
       " 'few',\n",
       " 'is not',\n",
       " 'every',\n",
       " 'lot',\n",
       " 'part',\n",
       " 'us',\n",
       " 'director',\n",
       " 'br i',\n",
       " 'again',\n",
       " 'thats',\n",
       " 'cant',\n",
       " 'the same',\n",
       " 'a good',\n",
       " 'want',\n",
       " 'cast',\n",
       " 'quite',\n",
       " 'there are',\n",
       " 'things',\n",
       " 'the end',\n",
       " 'seems',\n",
       " 'pretty',\n",
       " 'got',\n",
       " 'a lot',\n",
       " 'young',\n",
       " 'fact',\n",
       " 'the most',\n",
       " 'around',\n",
       " 'down',\n",
       " 'take',\n",
       " 'however',\n",
       " 'the best',\n",
       " 'world',\n",
       " 'but the',\n",
       " 'give',\n",
       " 'both',\n",
       " 'enough',\n",
       " 'between',\n",
       " 'own',\n",
       " 'may',\n",
       " 'ive',\n",
       " 'big',\n",
       " 'horror',\n",
       " 'original',\n",
       " 'about the',\n",
       " 'thought',\n",
       " 'to make',\n",
       " 'of his',\n",
       " 'always',\n",
       " 'without',\n",
       " 'series',\n",
       " 'the only',\n",
       " 'gets',\n",
       " 'i think',\n",
       " 'right',\n",
       " 'on a',\n",
       " 'but i',\n",
       " 'he is',\n",
       " 'long',\n",
       " 'saw',\n",
       " 'almost',\n",
       " 'isnt',\n",
       " 'that is',\n",
       " 'come',\n",
       " 'least',\n",
       " 'role',\n",
       " 'times',\n",
       " 'action',\n",
       " 'whole',\n",
       " 'theres',\n",
       " 'that i',\n",
       " 'a movie',\n",
       " 'point',\n",
       " 'interesting',\n",
       " 'comedy',\n",
       " 'must',\n",
       " 'family',\n",
       " 'a great',\n",
       " 'bit',\n",
       " 'to watch',\n",
       " 'the plot',\n",
       " 'hes',\n",
       " 'this one',\n",
       " 'done',\n",
       " 'music',\n",
       " 'script',\n",
       " 'but it',\n",
       " 'the characters',\n",
       " 'a very',\n",
       " 'to a',\n",
       " 'anything',\n",
       " 'feel',\n",
       " 'since',\n",
       " 'minutes',\n",
       " 'might',\n",
       " 'last',\n",
       " 'far',\n",
       " 'that it',\n",
       " 'performance',\n",
       " 'probably',\n",
       " 'guy',\n",
       " 'a few',\n",
       " 'be a',\n",
       " 'am',\n",
       " 'some of',\n",
       " 'kind',\n",
       " 'and it',\n",
       " 'was the',\n",
       " 'i dont',\n",
       " 'rather',\n",
       " 'have to',\n",
       " 'away',\n",
       " 'to get',\n",
       " 'worst',\n",
       " 'yet',\n",
       " 'sure',\n",
       " 'its a',\n",
       " 'woman',\n",
       " 'having',\n",
       " 'a little',\n",
       " 'tv',\n",
       " 'each',\n",
       " 'want to',\n",
       " 'girl',\n",
       " 'found',\n",
       " 'making',\n",
       " 'they are',\n",
       " 'anyone',\n",
       " 'would have',\n",
       " 'played',\n",
       " 'fun',\n",
       " 'is that',\n",
       " 'trying',\n",
       " 'into the',\n",
       " 'at least',\n",
       " 'course',\n",
       " 'comes',\n",
       " 'like a',\n",
       " 'our',\n",
       " 'the acting',\n",
       " 'i am',\n",
       " 'although',\n",
       " 'to do',\n",
       " 'the other',\n",
       " 'has a',\n",
       " 'believe',\n",
       " 'especially',\n",
       " 'the way',\n",
       " 'lot of',\n",
       " 'hard',\n",
       " 'day',\n",
       " 'goes',\n",
       " 'looks',\n",
       " 'wasnt',\n",
       " 'shows',\n",
       " 'different',\n",
       " 'a film',\n",
       " 'is one',\n",
       " 'kind of',\n",
       " 'maybe',\n",
       " 'would be',\n",
       " 'i would',\n",
       " 'set',\n",
       " 'this was',\n",
       " 'br this',\n",
       " 'place',\n",
       " 'someone',\n",
       " 'true',\n",
       " 'to have',\n",
       " 'put',\n",
       " 'watched',\n",
       " 'sense',\n",
       " 'have a',\n",
       " 'worth',\n",
       " 'main',\n",
       " '2',\n",
       " 'reason',\n",
       " 'trying to',\n",
       " 'money',\n",
       " 'ending',\n",
       " 'book',\n",
       " 'actor',\n",
       " 'once',\n",
       " 'the whole',\n",
       " 'everything',\n",
       " 'looking',\n",
       " 'job',\n",
       " 'plays',\n",
       " 'he was',\n",
       " 'seem',\n",
       " 'of course',\n",
       " 'dvd',\n",
       " 'that this',\n",
       " 'as well',\n",
       " 'said',\n",
       " 'three',\n",
       " 'john',\n",
       " 'like the',\n",
       " 'and his',\n",
       " 'later',\n",
       " 'instead',\n",
       " 'takes',\n",
       " 'screen',\n",
       " '10',\n",
       " 'effects',\n",
       " 'left',\n",
       " 'together',\n",
       " 'in his',\n",
       " 'himself',\n",
       " 'beautiful',\n",
       " 'during',\n",
       " 'a bit',\n",
       " 'version',\n",
       " 'you can',\n",
       " 'seeing',\n",
       " 'movie was',\n",
       " 'play',\n",
       " 'everyone',\n",
       " 'house',\n",
       " 'most of',\n",
       " 'the original',\n",
       " 'audience',\n",
       " 'the worst',\n",
       " 'special',\n",
       " 'excellent',\n",
       " 'idea',\n",
       " 'night',\n",
       " 'is an',\n",
       " 'that he',\n",
       " 'american',\n",
       " 'not a',\n",
       " 'could have',\n",
       " 'simply',\n",
       " 'which is',\n",
       " 'i can',\n",
       " 'the time',\n",
       " 'nice',\n",
       " 'more than',\n",
       " 'shot',\n",
       " 'completely',\n",
       " 'to say',\n",
       " 'read',\n",
       " 'going to',\n",
       " 'wife',\n",
       " 'second',\n",
       " 'i had',\n",
       " 'less',\n",
       " 'star',\n",
       " 'high',\n",
       " 'help',\n",
       " 'black',\n",
       " 'by a',\n",
       " 'kids',\n",
       " 'who is',\n",
       " 'youre',\n",
       " 'else',\n",
       " 'poor',\n",
       " 'fan',\n",
       " 'year',\n",
       " 'used',\n",
       " 'all of',\n",
       " 'at all',\n",
       " 'given',\n",
       " 'war',\n",
       " 'enjoy',\n",
       " 'i saw',\n",
       " 'home',\n",
       " 'when i',\n",
       " 'performances',\n",
       " 'father',\n",
       " 'try',\n",
       " 'but this',\n",
       " 'until',\n",
       " 'death',\n",
       " 'use',\n",
       " 'friends',\n",
       " 'need',\n",
       " 'rest',\n",
       " 'when the',\n",
       " 'classic',\n",
       " 'into a',\n",
       " 'of all',\n",
       " 'either',\n",
       " 'seems to',\n",
       " 'short',\n",
       " 'is so',\n",
       " 'such a',\n",
       " 'and then',\n",
       " 'truly',\n",
       " 'wrong',\n",
       " 'mind',\n",
       " 'men',\n",
       " 'along',\n",
       " 'next',\n",
       " 'of it',\n",
       " 'it has',\n",
       " 'half',\n",
       " 'the fact',\n",
       " 'production',\n",
       " 'its not',\n",
       " 'of them',\n",
       " 'hollywood',\n",
       " 'boring',\n",
       " 'dead',\n",
       " 'tell',\n",
       " 'women',\n",
       " 'movie and',\n",
       " 'remember',\n",
       " 'perhaps',\n",
       " 'part of',\n",
       " 'line',\n",
       " 'couple',\n",
       " 'start',\n",
       " 'came',\n",
       " 'recommend',\n",
       " 'in my',\n",
       " 'is just',\n",
       " 'full',\n",
       " 'let',\n",
       " 'i thought',\n",
       " 'episode',\n",
       " 'she is',\n",
       " 'moments',\n",
       " 'with his',\n",
       " 'like this',\n",
       " 'the rest',\n",
       " 'understand',\n",
       " 'movie i',\n",
       " 'mean',\n",
       " 'getting',\n",
       " 'awful',\n",
       " 'stupid',\n",
       " 'wonderful',\n",
       " 'camera',\n",
       " 'doing',\n",
       " 'fact that',\n",
       " 'the main',\n",
       " 'others',\n",
       " 'as it',\n",
       " 'often',\n",
       " 'early',\n",
       " 'terrible',\n",
       " 'playing',\n",
       " 'video',\n",
       " 'definitely',\n",
       " 'keep',\n",
       " 'gives',\n",
       " 'small',\n",
       " 'you have',\n",
       " 'is very',\n",
       " 'for this',\n",
       " 'finally',\n",
       " 'sex',\n",
       " 'from a',\n",
       " 'the world',\n",
       " 'stars',\n",
       " 'become',\n",
       " 'film and',\n",
       " 'perfect',\n",
       " 'the actors',\n",
       " 'school',\n",
       " 'name',\n",
       " 'that was',\n",
       " 'itself',\n",
       " 'felt',\n",
       " 'couldnt',\n",
       " 'yes',\n",
       " 'they were',\n",
       " 'over the',\n",
       " 'has been',\n",
       " 'case',\n",
       " 'absolutely',\n",
       " 'supposed',\n",
       " 'that they',\n",
       " 'face',\n",
       " 'lost',\n",
       " 'top',\n",
       " 'human',\n",
       " 'the two',\n",
       " 'you are',\n",
       " 'piece',\n",
       " 'person',\n",
       " 'lines',\n",
       " 'went',\n",
       " 'liked',\n",
       " 'dialogue',\n",
       " 'title',\n",
       " 'the director',\n",
       " 'through the',\n",
       " 'had a',\n",
       " 'there was',\n",
       " 'written',\n",
       " 'live',\n",
       " 'against',\n",
       " 'it and',\n",
       " 'sort',\n",
       " 'shes',\n",
       " 'certainly',\n",
       " 'the last',\n",
       " 'entire',\n",
       " 'it to',\n",
       " 'and that',\n",
       " 'in fact',\n",
       " 'problem',\n",
       " 'budget',\n",
       " 'of her',\n",
       " 'be the',\n",
       " '3',\n",
       " 'worse',\n",
       " 'the script',\n",
       " 'style',\n",
       " 'several',\n",
       " 'it a',\n",
       " 'head',\n",
       " 'mr',\n",
       " 'see the',\n",
       " 'fans',\n",
       " 'hope',\n",
       " 'at a',\n",
       " 'of my',\n",
       " 'id',\n",
       " 'waste',\n",
       " 'loved',\n",
       " 'and is',\n",
       " 'the show',\n",
       " 'overall',\n",
       " 'picture',\n",
       " 'story is',\n",
       " 'already',\n",
       " 'cinema',\n",
       " 'it would',\n",
       " 'example',\n",
       " 'so much',\n",
       " 'will be',\n",
       " 'he has',\n",
       " 'and he',\n",
       " 'to find',\n",
       " 'entertaining',\n",
       " 'evil',\n",
       " 'about this',\n",
       " 'care',\n",
       " 'seemed',\n",
       " 'despite',\n",
       " 'boy',\n",
       " '\\x96',\n",
       " 'oh',\n",
       " 'based',\n",
       " 'friend',\n",
       " 'i cant',\n",
       " 'white',\n",
       " 'rest of',\n",
       " 'can be',\n",
       " 'beginning',\n",
       " 'becomes',\n",
       " 'wanted',\n",
       " 'lives',\n",
       " 'killer',\n",
       " 'final',\n",
       " 'unfortunately',\n",
       " 'is in',\n",
       " 'supposed to',\n",
       " 'direction',\n",
       " 'dark',\n",
       " 'not to',\n",
       " 'turn',\n",
       " 'guys',\n",
       " 'does not',\n",
       " 'just a',\n",
       " 'is no',\n",
       " 'totally',\n",
       " 'mother',\n",
       " 'movie that',\n",
       " 'and its',\n",
       " 'fine',\n",
       " 'film was',\n",
       " 'throughout',\n",
       " 'sort of',\n",
       " 'should be',\n",
       " 'the cast',\n",
       " 'wants',\n",
       " 'with this',\n",
       " 'film that',\n",
       " '1',\n",
       " 'to this',\n",
       " 'to go',\n",
       " 'lead',\n",
       " 'sound',\n",
       " 'humor',\n",
       " 'children',\n",
       " 'wont',\n",
       " 'history',\n",
       " 'guess',\n",
       " 'as i',\n",
       " 'under',\n",
       " 'writing',\n",
       " 'tries',\n",
       " 'because of',\n",
       " 'girls',\n",
       " 'called',\n",
       " 'michael',\n",
       " 'laugh',\n",
       " 'works',\n",
       " 'about a',\n",
       " 'not the',\n",
       " 'i could',\n",
       " 'drama',\n",
       " 'than the',\n",
       " 'enjoyed',\n",
       " 'and this',\n",
       " 'you will',\n",
       " 'i found',\n",
       " 'turns',\n",
       " 'amazing',\n",
       " 'past',\n",
       " 'able',\n",
       " 'watch it',\n",
       " 'theyre',\n",
       " 'hard to',\n",
       " 'days',\n",
       " 'youll',\n",
       " 'act',\n",
       " 'watch this',\n",
       " 'low',\n",
       " 'the book',\n",
       " 'has to',\n",
       " 'the films',\n",
       " 'behind',\n",
       " 'quality',\n",
       " 'end of',\n",
       " 'ever seen',\n",
       " 'gave',\n",
       " 'favorite',\n",
       " 'kill',\n",
       " 'i didnt',\n",
       " 'to his',\n",
       " 'was not',\n",
       " 'when he',\n",
       " 'the audience',\n",
       " 'see this',\n",
       " 'son',\n",
       " 'game',\n",
       " 'able to',\n",
       " 'is also',\n",
       " 'sometimes',\n",
       " 'make a',\n",
       " 'had to',\n",
       " 'starts',\n",
       " 'side',\n",
       " 'town',\n",
       " 'such as',\n",
       " 'see it',\n",
       " 'car',\n",
       " 'horrible',\n",
       " 'art',\n",
       " 'after the',\n",
       " 'because it',\n",
       " 'up to',\n",
       " 'actress',\n",
       " 'for me',\n",
       " 'say that',\n",
       " 'parts',\n",
       " 'where the',\n",
       " 'viewer',\n",
       " 'eyes',\n",
       " 'as an',\n",
       " 'should have',\n",
       " 'i really',\n",
       " 'expect',\n",
       " 'but its',\n",
       " 'do not',\n",
       " 'acting is',\n",
       " 'not only',\n",
       " 'have seen',\n",
       " 'well as',\n",
       " 'themselves',\n",
       " 'stories',\n",
       " 'a bad',\n",
       " 'i just',\n",
       " 'ones',\n",
       " 'obviously',\n",
       " 'the ending',\n",
       " 'and not',\n",
       " 'of their',\n",
       " 'child',\n",
       " 'run',\n",
       " 'flick',\n",
       " 'thinking',\n",
       " 'it i',\n",
       " 'in which',\n",
       " 'soon',\n",
       " 'seem to',\n",
       " 'feeling',\n",
       " 'very good',\n",
       " 'heart',\n",
       " 'to me',\n",
       " 'so i',\n",
       " 'on this',\n",
       " 'directed',\n",
       " 'back to',\n",
       " 'of those',\n",
       " 'did not',\n",
       " 'decent',\n",
       " 'in an',\n",
       " 'ill',\n",
       " 'ive seen',\n",
       " 'i know',\n",
       " 'took',\n",
       " 'except',\n",
       " 'late',\n",
       " 'that you',\n",
       " 'highly',\n",
       " 'genre',\n",
       " 'myself',\n",
       " 'cannot',\n",
       " 'better than',\n",
       " 'are the',\n",
       " 'you want',\n",
       " 'to take',\n",
       " 'close',\n",
       " 'heard',\n",
       " 'up with',\n",
       " 'city',\n",
       " 'brilliant',\n",
       " 'stuff',\n",
       " 'says',\n",
       " 'fight',\n",
       " 'blood',\n",
       " 'hell',\n",
       " 'in their',\n",
       " 'enough to',\n",
       " 'are a',\n",
       " 'a couple',\n",
       " 'extremely',\n",
       " 'wouldnt',\n",
       " 'killed',\n",
       " 'so many',\n",
       " 'the entire',\n",
       " 'each other',\n",
       " 'kid',\n",
       " 'and they',\n",
       " 'played by',\n",
       " 'matter',\n",
       " 'in all',\n",
       " 'lack',\n",
       " 'it the',\n",
       " 'of an',\n",
       " 'during the',\n",
       " 'special effects',\n",
       " 'leave',\n",
       " 'moment',\n",
       " 'what i',\n",
       " 'hand',\n",
       " 'told',\n",
       " 'roles',\n",
       " 'particularly',\n",
       " 'hour',\n",
       " 'the one',\n",
       " 'happened',\n",
       " 'strong',\n",
       " 'wonder',\n",
       " 'with her',\n",
       " 'happens',\n",
       " 'tries to',\n",
       " 'obvious',\n",
       " 'based on',\n",
       " 'a big',\n",
       " 'involved',\n",
       " 'this show',\n",
       " 'attempt',\n",
       " 'police',\n",
       " 'etc',\n",
       " 'violence',\n",
       " 'when it',\n",
       " 'film i',\n",
       " 'are not',\n",
       " 'was so',\n",
       " 'about it',\n",
       " 'too much',\n",
       " 'if the',\n",
       " 'what the',\n",
       " 'she was',\n",
       " 'story of',\n",
       " 'including',\n",
       " 'is about',\n",
       " 'save',\n",
       " 'please',\n",
       " 'dont know',\n",
       " 'and even',\n",
       " 'living',\n",
       " 'instead of',\n",
       " 'murder',\n",
       " 'piece of',\n",
       " 'number',\n",
       " 'i love',\n",
       " 'and in',\n",
       " 'chance',\n",
       " 'anyway',\n",
       " 'itbr br',\n",
       " 'itbr',\n",
       " 'age',\n",
       " 'movie the',\n",
       " 'in it',\n",
       " 'as he',\n",
       " 'a man',\n",
       " 'score',\n",
       " 'group',\n",
       " 'br it',\n",
       " 'looked',\n",
       " 'lets',\n",
       " 'complete',\n",
       " 'coming',\n",
       " 'experience',\n",
       " 'james',\n",
       " 'it all',\n",
       " 'shown',\n",
       " 'happen',\n",
       " 'alone',\n",
       " 'usually',\n",
       " 'i watched',\n",
       " 'what is',\n",
       " 'none',\n",
       " 'running',\n",
       " 'out the',\n",
       " 'simple',\n",
       " 'ago',\n",
       " 'a real',\n",
       " 'taken',\n",
       " 'god',\n",
       " 'film the',\n",
       " 'type',\n",
       " 'his own',\n",
       " 'ends',\n",
       " 'of these',\n",
       " 'even the',\n",
       " 'slow',\n",
       " 'stop',\n",
       " 'ok',\n",
       " 'serious',\n",
       " 'exactly',\n",
       " 'and you',\n",
       " 'who has',\n",
       " 'the very',\n",
       " 'of its',\n",
       " 'im not',\n",
       " 'if it',\n",
       " 'song',\n",
       " 'even though',\n",
       " 'daughter',\n",
       " 'br in',\n",
       " 'wish',\n",
       " 'whose',\n",
       " 'musical',\n",
       " 'david',\n",
       " 'people who',\n",
       " 'sense of',\n",
       " 'released',\n",
       " 'watching this',\n",
       " 'sad',\n",
       " 'career',\n",
       " 'been a',\n",
       " 'known',\n",
       " 'usual',\n",
       " 'but not',\n",
       " 'hours',\n",
       " 'fan of',\n",
       " 'cinematography',\n",
       " 'try to',\n",
       " 'interest',\n",
       " 'the music',\n",
       " 'seriously',\n",
       " 'i mean',\n",
       " 'have the',\n",
       " 'finds',\n",
       " 'novel',\n",
       " 'across',\n",
       " 'wanted to',\n",
       " 'huge',\n",
       " 'crap',\n",
       " 'what a',\n",
       " 'br if',\n",
       " 'voice',\n",
       " 'the beginning',\n",
       " 'scary',\n",
       " 'as if',\n",
       " '4',\n",
       " 'opening',\n",
       " 'major',\n",
       " 'jokes',\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:20:06.295926Z",
     "start_time": "2025-11-13T13:20:06.170932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bigram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "bigram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "bigram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
   ],
   "id": "ecbb77b75911af4e",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:20:27.143798Z",
     "start_time": "2025-11-13T13:20:27.041798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x, y = next(bigram_train_ds.as_numpy_iterator())\n",
    "x.shape"
   ],
   "id": "f06ec00b680331d0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 30000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:22:03.878266Z",
     "start_time": "2025-11-13T13:21:23.662541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = build_linear_classifier(max_tokens, \"bigram_classifier\")\n",
    "model.fit(\n",
    "    bigram_train_ds,\n",
    "    validation_data=bigram_val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ],
   "id": "52c300b169be7955",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 6s 10ms/step - loss: 0.3988 - accuracy: 0.8654 - val_loss: 0.3062 - val_accuracy: 0.8940\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.2232 - accuracy: 0.9356 - val_loss: 0.2673 - val_accuracy: 0.9022\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 5s 9ms/step - loss: 0.1639 - accuracy: 0.9605 - val_loss: 0.2529 - val_accuracy: 0.9060\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 5s 9ms/step - loss: 0.1274 - accuracy: 0.9749 - val_loss: 0.2475 - val_accuracy: 0.9074\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.1015 - accuracy: 0.9841 - val_loss: 0.2459 - val_accuracy: 0.9072\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.0822 - accuracy: 0.9895 - val_loss: 0.2472 - val_accuracy: 0.9072\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 5s 9ms/step - loss: 0.0671 - accuracy: 0.9937 - val_loss: 0.2501 - val_accuracy: 0.9062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x22b8c3d91e0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:22:40.893021Z",
     "start_time": "2025-11-13T13:22:35.437631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loss, test_acc = model.evaluate(bigram_test_ds)\n",
    "test_acc"
   ],
   "id": "de0b2bfd0c7ed3ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 5s 7ms/step - loss: 0.2506 - accuracy: 0.9010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9010000228881836"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 序列模型",
   "id": "86b8d561bc734f83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# \"the quick brown fox jumped over the lazy dog\"\n",
    "#\n",
    "# \"the slow brown badger\"\n",
    "#\n",
    "# [\"the\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"lazy\"]\n",
    "# [\"the\", \"slow\", \"brown\", \"badger\", \"[PAD]\", \"[PAD]\", \"[PAD]\", \"[PAD]\"]"
   ],
   "id": "90af96b74f278529"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:36:27.573582Z",
     "start_time": "2025-11-13T15:36:23.193769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_length = 600\n",
    "max_tokens = 30_000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens =max_tokens,\n",
    "    split=\"whitespace\",\n",
    "    output_mode=\"int\",\n",
    "\n",
    "    # pads / truncates to 600 tokens\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "text_vectorization.adapt(train_ds_no_labels)\n",
    "\n",
    "sequence_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y)\n",
    ")\n",
    "sequence_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y)\n",
    ")\n",
    "sequence_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y)\n",
    ")"
   ],
   "id": "b98e7270928a7052",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:36:31.297255Z",
     "start_time": "2025-11-13T15:36:31.206249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x, y = next(sequence_test_ds.as_numpy_iterator())\n",
    "x.shape\n",
    "x"
   ],
   "id": "7a9afc3bddbc6a44",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 600)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 训练循环网络",
   "id": "859eca169861c182"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在每个时间步，常规循环层在生成输出之前只查看过去和现在的输入，这意味着无法看到未来。为了依据未来的输入去决定现在的编码，一种解决方案是在相同的输入上运行两个循环层（一个从左到右读取单词，另一个从右到左读取它们），然后在每个时间步组合它们的输出（直接连接起来，直接参与该时间步的预测）\n",
    "\n",
    "![双向RNN](./images/RNN/p8.png)"
   ],
   "id": "668c57f279582b6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T14:18:57.353947Z",
     "start_time": "2025-11-13T14:18:57.339949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "\n",
    "class OneHotEncoding(Layer):\n",
    "    def __init__(self, depth, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.depth = depth\n",
    "\n",
    "    def call(self, inputs):\n",
    "       return K.one_hot(K.cast(inputs, \"int32\"), self.depth)\n",
    "\n",
    "one_hot_encoding = OneHotEncoding(max_tokens)"
   ],
   "id": "4627c439c3552e74",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T14:18:58.399852Z",
     "start_time": "2025-11-13T14:18:57.799974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x, y = next(sequence_train_ds.as_numpy_iterator())\n",
    "one_hot_encoding(x).shape"
   ],
   "id": "a5b4f592ed57ddc1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 600, 30000])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T14:19:00.491225Z",
     "start_time": "2025-11-13T14:18:59.958475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hidden_dim = 64\n",
    "inputs = keras.Input(shape=(max_length,), dtype=\"int32\")\n",
    "x= one_hot_encoding(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(hidden_dim))(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs, name=\"lstm_with_one_hot\")\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ],
   "id": "2f6ac1d8e7c882a0",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T14:19:13.536465Z",
     "start_time": "2025-11-13T14:19:13.507321Z"
    }
   },
   "cell_type": "code",
   "source": "model.summary()",
   "id": "70566e88b775337b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lstm_with_one_hot\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 600)]             0         \n",
      "                                                                 \n",
      " one_hot_encoding_3 (OneHot  (None, 600, 30000)        0         \n",
      " Encoding)                                                       \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 128)               15393280  \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15393409 (58.72 MB)\n",
      "Trainable params: 15393409 (58.72 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-11-13T15:19:20.883853Z",
     "start_time": "2025-11-13T14:19:28.513703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# WARNING: 训练时间非常长\n",
    "model.fit(\n",
    "    sequence_train_ds,\n",
    "    validation_data=sequence_val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ],
   "id": "de6346c855fcf7f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "169/625 [=======>......................] - ETA: 2:40:05 - loss: 0.6832 - accuracy: 0.5636"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_loss, test_acc = model.evaluate(sequence_test_ds)\n",
    "test_acc"
   ],
   "id": "b4f1a8b2515f37d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It’s common to see word embeddings that are 256-dimensional, 512-dimensional, or 1,024-dimensional when dealing with very large vocabularies. On the other hand, one-hot encoding words generally leads to vectors that are 30,000-dimensional in the case of our current vocabulary. So word embeddings pack more information into far fewer dimensions.\n",
    "\n",
    "![token到embedding](./images/RNN/p9.png)\n",
    "\n",
    "The Embedding layer takes as input a rank-2 tensor with shape (batch_size, sequence_length), where each entry is a sequence of integers. The layer returns a floating-point tensor of shape (batch_size, sequence_length, embedding_size)."
   ],
   "id": "9f43dfad09e04eb8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:36:58.039418Z",
     "start_time": "2025-11-13T15:36:57.109015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hidden_dim = 64\n",
    "\n",
    "inputs = keras.Input(shape=(max_length,), dtype=\"int32\")\n",
    "\n",
    "x = keras.layers.Embedding(\n",
    "    input_dim=max_tokens,\n",
    "    output_dim=hidden_dim,\n",
    "    mask_zero=True, # mask all elements that initially conatained a zero\n",
    ")(inputs)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(hidden_dim))(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs, name=\"lstm_with_embedding\")\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ],
   "id": "903b9b7a7ba3dcf4",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:46:52.642395Z",
     "start_time": "2025-11-13T15:36:59.751444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.fit(\n",
    "    sequence_train_ds,\n",
    "    validation_data=sequence_val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ],
   "id": "41dc53fd33c448c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 121s 188ms/step - loss: 0.4470 - accuracy: 0.7876 - val_loss: 0.3513 - val_accuracy: 0.8472\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 116s 186ms/step - loss: 0.2420 - accuracy: 0.9062 - val_loss: 0.3396 - val_accuracy: 0.8718\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 118s 188ms/step - loss: 0.1161 - accuracy: 0.9586 - val_loss: 0.3314 - val_accuracy: 0.8704\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 119s 190ms/step - loss: 0.0626 - accuracy: 0.9801 - val_loss: 0.4099 - val_accuracy: 0.8620\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 118s 189ms/step - loss: 0.0404 - accuracy: 0.9872 - val_loss: 0.4685 - val_accuracy: 0.8658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x188af35ba60>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_loss, test_acc = model.evaluate(sequence_test_ds)\n",
    "test_acc"
   ],
   "id": "c1cfcf1234ee806f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 预训练词嵌入",
   "id": "abe26f988fd5070f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:50:32.363490Z",
     "start_time": "2025-11-13T15:50:32.147381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "imdb_vocabulary = text_vectorization.get_vocabulary()\n",
    "tokenize_no_padding = keras.layers.TextVectorization(\n",
    "    vocabulary=imdb_vocabulary,\n",
    "    split=\"whitespace\",\n",
    "    output_mode=\"int\"\n",
    ")"
   ],
   "id": "2f75f4b5598b8694",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:53:14.638435Z",
     "start_time": "2025-11-13T15:53:09.322758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Words to the left or right of label\n",
    "context_size = 4\n",
    "# Total window size\n",
    "window_size = 9\n",
    "\n",
    "def window_data(token_ids):\n",
    "    num_windows = tf.maximum(tf.size(token_ids) - context_size * 2, 0)\n",
    "    windows = tf.range(window_size)[None, :]\n",
    "    windows = windows + tf.range(num_windows)[:, None]\n",
    "    windowed_tokens = tf.gather(token_ids, windows)\n",
    "    return tf.data.Dataset.from_tensor_slices(windowed_tokens)\n",
    "\n",
    "def split_label(window):\n",
    "    left = window[:context_size]\n",
    "    right = window[context_size + 1 :]\n",
    "    bag = tf.concat((left, right), axis=0)\n",
    "    label = window[context_size]\n",
    "    return bag, label\n",
    "\n",
    "# Uses all training data, including the unsup/ directory\n",
    "dataset = keras.utils.text_dataset_from_directory(\n",
    "    imdb_extract_dir / \"train\", batch_size=None\n",
    ")\n",
    "# Drops label\n",
    "dataset = dataset.map(lambda x, y: x, num_parallel_calls=8)\n",
    "# Tokenizes\n",
    "dataset = dataset.map(tokenize_no_padding, num_parallel_calls=8)\n",
    "# Creates context windows\n",
    "dataset = dataset.interleave(window_data, cycle_length=8, num_parallel_calls=8)\n",
    "# Splits middle wonder into a label\n",
    "dataset = dataset.map(split_label, num_parallel_calls=8)"
   ],
   "id": "aa0ec15b54176f9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75000 files belonging to 3 classes.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:58:27.010121Z",
     "start_time": "2025-11-13T15:58:26.966220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hidden_dim = 64\n",
    "inputs = keras.Input(shape=(2 * context_size,))\n",
    "cbow_embedding = layers.Embedding(\n",
    "    max_tokens,\n",
    "    hidden_dim,\n",
    ")\n",
    "x = cbow_embedding(inputs)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(max_tokens, activation=\"sigmoid\")(x)\n",
    "cbow_model = keras.Model(inputs, outputs)\n",
    "cbow_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")"
   ],
   "id": "fcba7dee4ae790f5",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T15:58:32.020700Z",
     "start_time": "2025-11-13T15:58:31.994773Z"
    }
   },
   "cell_type": "code",
   "source": "cbow_model.summary()",
   "id": "ecbe1ed912943ca1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, 8, 64)             1920000   \n",
      "                                                                 \n",
      " global_average_pooling1d_1  (None, 64)                0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 30000)             1950000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3870000 (14.76 MB)\n",
      "Trainable params: 3870000 (14.76 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-13T15:58:38.720399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = dataset.batch(1024).cache()\n",
    "cbow_model.fit(dataset, epochs=4)"
   ],
   "id": "e2bca54171db2a02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "   3087/Unknown - 695s 225ms/step - loss: 6.9662 - sparse_categorical_accuracy: 0.0613"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "inputs = keras.Input(shape=(max_length,))\n",
    "lstm_embedding = layers.Embedding(\n",
    "    input_dim=max_tokens,\n",
    "    output_dim=hidden_dim,\n",
    "    mask_zero=True,\n",
    ")\n",
    "x = lstm_embedding(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(hidden_dim))(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs, name=\"lstm_with_cbow\")\n",
    "\n",
    "lstm_embedding.embeddings.assign(cbow_embedding.embeddings)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.fit(\n",
    "    sequence_train_ds,\n",
    "    validation_data=sequence_val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ],
   "id": "8d8e680ce271e8e0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl3",
   "language": "python",
   "name": "homl3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
